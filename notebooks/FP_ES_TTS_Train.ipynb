{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f018747c",
      "metadata": {
        "id": "f018747c"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import nemo\n",
        "import torch\n",
        "import librosa\n",
        "import numpy as np\n",
        "\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "d9e63f12",
      "metadata": {
        "id": "d9e63f12"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "74b10183",
      "metadata": {
        "id": "74b10183"
      },
      "source": [
        "### FastPitch\n",
        "\n",
        "FastPitch is non-autoregressive model for mel-spectrogram generation based on FastSpeech, conditioned on fundamental frequency contours. For more details about model, please refer to the original [paper](https://ieeexplore.ieee.org/abstract/document/9413889). Original [FastPitch model](https://ieeexplore.ieee.org/abstract/document/9413889) uses an external Tacotron 2 model trained on LJSpeech-1.1 to extract training alignments and estimate durations of input symbols. This implementation of FastPitch is based on [Deep Learning Examples](https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/SpeechSynthesis/FastPitch), which uses an alignment mechanism proposed in [RAD-TTS](https://openreview.net/pdf?id=0NQwnnwAORi) and extended in [TTS Aligner](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9747707).\n",
        "\n",
        "For more information on training a basic FastPitch model, please refer to [FastPitch_MixerTTS_Training.ipynb](https://github.com/NVIDIA/NeMo/blob/main/tutorials/tts/FastPitch_MixerTTS_Training.ipynb) tutorial.\n",
        "\n",
        "### HiFiGAN\n",
        "HiFiGAN is a generative adversarial network (GAN) model that generates audio from mel spectrograms. The generator uses transposed convolutions to upsample mel spectrograms to audio. For more details about the model, please refer to the original [paper](https://arxiv.org/abs/2010.05646). NeMo re-implementation of HiFi-GAN can be found [here](https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/tts/models/hifigan.py)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "f8b533b7",
      "metadata": {
        "id": "f8b533b7"
      },
      "source": [
        "# Dataset Preparation"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "6832355f",
      "metadata": {},
      "source": [
        "### Preparado en Script aparte\n",
        "\n",
        "Ahora tenemos que ejecutar el script que figura a continuaci칩n, que deber치 terminar de arreglarse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3f20f34",
      "metadata": {},
      "outputs": [],
      "source": [
        "#!(cd /home/irene/notebooks && bash fptts-getdata.sh)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "be06b756",
      "metadata": {},
      "source": [
        "### Entrenamiento\n",
        "\n",
        "Se realiza en el script fastpitch, que se ejecuta desde el script que figura a continuaci칩n, que lo ejecutamos a mano .. *Es importante* saber que la memoria compartida debe ser m치s grande que el valor por defecto. Para eso, el contenedor en que estamos ejecutando todo esto debe ejecutarse con ```--shm-size=1g``` al menos.\n",
        "\n",
        "\n",
        "Ver /home/irene/nvcr.io/run.sh"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "fe0bfbb9",
      "metadata": {
        "id": "fe0bfbb9"
      },
      "source": [
        "Note:\n",
        "1. We use `CUDA_VISIBLE_DEVICES=0` to limit training to single GPU.\n",
        "2. For debugging you may also add the following flags: `HYDRA_FULL_ERROR=1`, `CUDA_LAUNCH_BLOCKING=1`\n",
        "\n",
        "<b>Note</b>: We've limited the above run to 1 epoch only, so we can validate the implementation within the scope of this tutorial. We recommend around 5000 epochs when training FastPitch from scratch."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "632d51b0",
      "metadata": {
        "id": "632d51b0"
      },
      "source": [
        "## Evaluating FastPitch + pretrained HiFi-GAN\n",
        "\n",
        "Let's evaluate the quality of the FastPitch model generated so far using a HiFi-GAN model pre-trained on English."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "053a00a2",
      "metadata": {
        "id": "053a00a2"
      },
      "outputs": [],
      "source": [
        "import IPython.display as ipd\n",
        "from nemo.collections.tts.models import HifiGanModel, FastPitchModel\n",
        "from matplotlib.pyplot import imshow\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d82bc36b",
      "metadata": {
        "id": "d82bc36b"
      },
      "outputs": [],
      "source": [
        "test = \"das werden einmal \\u00fcber eine million kleiner buchenpflanzen werden.\" # text input to the model\n",
        "test_id = \"2b2b496ccc9b57130f559c4fd827825f\" # identifier for the audio corresponding to the test text\n",
        "data_path = \"data_thorsten_2210/ThorstenVoice-Dataset-22_10/ThorstenVoice-Dataset_2022.10/wavs/\" # path to dataset folder with wav files from original dataset\n",
        "seed = 1234"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87ff970d",
      "metadata": {
        "id": "87ff970d"
      },
      "outputs": [],
      "source": [
        "def evaluate_spec_fastpitch_ckpt(spec_gen_model, v_model, test):\n",
        "    with torch.no_grad():\n",
        "        torch.manual_seed(seed)\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.backends.cudnn.enabled = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "        parsed = spec_gen_model.parse(str_input=test, normalize=True)\n",
        "        spectrogram = spec_gen_model.generate_spectrogram(tokens=parsed)\n",
        "        print(spectrogram.size())\n",
        "        audio = v_model.convert_spectrogram_to_audio(spec=spectrogram)\n",
        "\n",
        "    spectrogram = spectrogram.to('cpu').numpy()[0]\n",
        "    audio = audio.to('cpu').numpy()[0]\n",
        "    audio = audio / np.abs(audio).max()\n",
        "    return audio, spectrogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70733af0",
      "metadata": {
        "id": "70733af0"
      },
      "outputs": [],
      "source": [
        "# load fastpitch and hifigan models\n",
        "import glob, os\n",
        "fastpitch_model_path = sorted(\n",
        "    glob.glob(\"NeMoGermanTTS/resultGermanTTS/FastPitch/*/checkpoints/FastPitch.nemo\"),\n",
        "    key=os.path.getmtime\n",
        ")[-1] # path_to_fastpitch_nemo_or_ckpt\n",
        "hfg_ngc = \"tts_en_lj_hifigan_ft_mixerttsx\" # NGC pretrained model name: https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/tts_en_lj_hifigan\n",
        "\n",
        "vocoder_model = HifiGanModel.from_pretrained(hfg_ngc, strict=False).eval().cuda()\n",
        "if \".nemo\" in fastpitch_model_path:\n",
        "    spec_gen_model = FastPitchModel.restore_from(fastpitch_model_path).eval().cuda()\n",
        "else:\n",
        "    spec_gen_model = FastPitchModel.load_from_checkpoint(checkpoint_path=fastpitch_model_path).eval().cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27aa1827",
      "metadata": {
        "id": "27aa1827"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "audio, spectrogram = evaluate_spec_fastpitch_ckpt(spec_gen_model, vocoder_model, test)\n",
        "\n",
        "# visualize the spectrogram\n",
        "if spectrogram is not None:\n",
        "    imshow(spectrogram, origin=\"lower\")\n",
        "    plt.show()\n",
        "\n",
        "# audio\n",
        "print(\"original audio\")\n",
        "ipd.display(ipd.Audio(data_path+test_id+'.wav', rate=22050))\n",
        "print(\"predicted audio\")\n",
        "ipd.display(ipd.Audio(audio, rate=22050))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "15006392",
      "metadata": {
        "id": "15006392"
      },
      "source": [
        "You would hear that the above synthesized audio quality is not as good as we expect. It would be improved after continuing to train 5000 epochs. But again,the quality is still not acceptable. A straightforward solution is to finetune the HiFi-GAN model following the tutorial [FastPitch_Finetuning.ipynb](FastPitch_Finetuning.ipynb). Let's try that out next!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "a7002b1e",
      "metadata": {
        "id": "a7002b1e"
      },
      "source": [
        "# Finetuning HiFi-GAN\n",
        "\n",
        "Improving speech quality by finetuning HiFi-GAN on synthesized mel-spectrograms from FastPitch."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "4eccd357",
      "metadata": {
        "id": "4eccd357"
      },
      "source": [
        "## Generating synthetic mels\n",
        "\n",
        "To generate mel-spectrograms from FastPitch, we can use `generate_spectrogram` method defined in `nemo/collections/tts/models/fastpitch.py`. However, the resulting spectrogram may be different from ground truth mel spectrogram, as shown below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5edb6fe",
      "metadata": {
        "id": "a5edb6fe"
      },
      "outputs": [],
      "source": [
        "test_audio_filepath = \"data_thorsten_2210/ThorstenVoice-Dataset-22_10/ThorstenVoice-Dataset_2022.10/wavs/43ee28172fe1a9d1eebd77bc09f03e51.wav\"\n",
        "test_audio_text = \"daran beteiligten sich einhundertachtzig kommunen und hochschulen.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "769f270a",
      "metadata": {
        "id": "769f270a"
      },
      "outputs": [],
      "source": [
        "from matplotlib.pyplot import imshow\n",
        "from nemo.collections.tts.models import FastPitchModel\n",
        "from matplotlib import pyplot as plt\n",
        "import librosa\n",
        "import librosa.display\n",
        "import torch\n",
        "import soundfile as sf\n",
        "import numpy as np\n",
        "from nemo.collections.tts.parts.utils.tts_dataset_utils import BetaBinomialInterpolator\n",
        "\n",
        "def load_wav(audio_file):\n",
        "    with sf.SoundFile(audio_file, 'r') as f:\n",
        "        samples = f.read(dtype='float32')\n",
        "    return samples.transpose()\n",
        "\n",
        "def plot_logspec(spec, axis=None):\n",
        "    librosa.display.specshow(\n",
        "        librosa.amplitude_to_db(spec, ref=np.max),\n",
        "        y_axis='linear',\n",
        "        x_axis=\"time\",\n",
        "        fmin=0,\n",
        "        fmax=8000,\n",
        "        ax=axis\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aef1470b",
      "metadata": {
        "id": "aef1470b"
      },
      "outputs": [],
      "source": [
        "spec_model = FastPitchModel.restore_from(fastpitch_model_path).eval().cuda()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "5a840dcf",
      "metadata": {
        "id": "5a840dcf"
      },
      "source": [
        "So we have 2 types of mel spectrograms that we can use for finetuning HiFi-GAN:\n",
        "\n",
        "### 1. Original mel spectrogram generated from original audio file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60d49e67",
      "metadata": {
        "id": "60d49e67"
      },
      "outputs": [],
      "source": [
        "print(\"loading original melspec\")\n",
        "y, sr = librosa.load(test_audio_filepath)\n",
        "# change n_fft, win_length, hop_length parameters below based on your specific config file\n",
        "spectrogram2 = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=1024, win_length=1024, hop_length=256)\n",
        "spectrogram = spectrogram2[ :80, :]\n",
        "print(\"spectrogram shape = \", spectrogram.shape)\n",
        "plot_logspec(spectrogram)\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "5641b7c4",
      "metadata": {
        "id": "5641b7c4"
      },
      "source": [
        "### 2. Mel spectrogram predicted from FastPitch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f448efd",
      "metadata": {
        "id": "5f448efd"
      },
      "outputs": [],
      "source": [
        "print(\"loading fastpitch melspec via generate_spectrogram\")\n",
        "with torch.no_grad():\n",
        "    text = spec_model.parse(test_audio_text, normalize=False)\n",
        "    spectrogram = spec_model.generate_spectrogram(\n",
        "      tokens=text,\n",
        "      speaker=None,\n",
        "    )\n",
        "spectrogram = spectrogram.to('cpu').numpy()[0]\n",
        "plot_logspec(spectrogram)\n",
        "print(\"spectrogram shape = \", spectrogram.shape)\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "5c240c79",
      "metadata": {
        "id": "5c240c79"
      },
      "source": [
        "**Note**: The above predicted spectrogram has the duration of 241 frames which is not equal to the ground truth 345 frames. In order to finetune HiFi-GAN we need mel spectrogram predicted from FastPitch with ground truth alignment and duration.\n",
        "\n",
        "### 2.1 Mel spectrogram predicted from FastPitch with groundtruth alignment and duration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddd0eef5",
      "metadata": {
        "id": "ddd0eef5"
      },
      "outputs": [],
      "source": [
        "print(\"loading fastpitch melspec via forward method with groundtruth alignment and duration\")\n",
        "with torch.no_grad():\n",
        "    device = spec_model.device\n",
        "    beta_binomial_interpolator = BetaBinomialInterpolator()\n",
        "    text = spec_model.parse(test_audio_text, normalize=False)\n",
        "    text_len = torch.tensor(text.shape[-1], dtype=torch.long, device=device).unsqueeze(0)\n",
        "    audio = load_wav(test_audio_filepath)\n",
        "    audio = torch.from_numpy(audio).unsqueeze(0).to(device)\n",
        "    audio_len = torch.tensor(audio.shape[1], dtype=torch.long, device=device).unsqueeze(0)\n",
        "    spect, spect_len = spec_model.preprocessor(input_signal=audio, length=audio_len)\n",
        "    attn_prior = torch.from_numpy(\n",
        "      beta_binomial_interpolator(spect_len.item(), text_len.item())\n",
        "    ).unsqueeze(0).to(text.device)\n",
        "    spectrogram = spec_model.forward(\n",
        "      text=text,\n",
        "      input_lens=text_len,\n",
        "      spec=spect,\n",
        "      mel_lens=spect_len,\n",
        "      attn_prior=attn_prior,\n",
        "      speaker=None,\n",
        "    )[0]\n",
        "spectrogram = spectrogram.to('cpu').numpy()[0]\n",
        "print(\"spectrogram shape = \", spectrogram.shape)\n",
        "plot_logspec(spectrogram)\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "a2c99521",
      "metadata": {
        "id": "a2c99521"
      },
      "source": [
        "In our experience,\n",
        "- Finetuning with #1 has artifacts from the original audio (noise) that get passed on as input to the vocoder resulting in artifacts in vocoder output in the form of noise.\n",
        "- <b> On the other hand, #2.1 (i.e. `Mel spectrogram predicted from FastPitch with groundtruth alignment and duration`) gives the best results because it enables HiFi-GAN to learn mel spectrograms generated by FastPitch as well as duration distributions closer to the real world (i.e. ground truth) durations. </b>\n",
        "\n",
        "From implementation perspective - we follow the same process described in [Finetuning FastPitch for a new speaker](FastPitch_Finetuning.ipynb) - i.e. take the latest checkpoint from FastPitch training and predict spectrograms for each of the input records in `train_manifest_text_normed.json`, `test_manifest_text_normed.json` and `val_manifest_text_normed.json`. NeMo provides an efficient script, [scripts/dataset_processing/tts/generate_mels.py](https://raw.githubusercontent.com/nvidia/NeMo/main/scripts/dataset_processing/tts/generate_mels.py), to generate Mel-spectrograms in the directory `NeMoGermanTTS/mels` and also create new JSON manifests with a suffix `_mel` by adding a new key `\"mel_filepath\"`. For example, `train_manifest_text_normed.json` corresponds to `train_manifest_text_normed_mel.json` saved in the same directory. You can run the following CLI to obtain the new JSON manifests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1dc3025",
      "metadata": {
        "id": "c1dc3025"
      },
      "outputs": [],
      "source": [
        "!cd NeMoGermanTTS && python generate_mels.py \\\n",
        "    --cpu \\\n",
        "    --fastpitch-model-ckpt {fastpitch_model_path.split(\"/\", maxsplit=1)[1]} \\\n",
        "    --input-json-manifests train_manifest_text_normed.json val_manifest_text_normed.json test_manifest_text_normed.json \\\n",
        "    --output-json-manifest-root ./"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "d4cb8e4a",
      "metadata": {
        "id": "d4cb8e4a"
      },
      "source": [
        "Revisiting how we implement #2.1 (i.e. Predicted mel spectrogram predicted from FastPitch with groundtruth alignment and duration):\n",
        "\n",
        "1. Notice above that we use audio from dataset (`audio` variable) to compute spectrogram length (`spect_len`):\n",
        "    ```python\n",
        "    spect, spect_len = spec_model.preprocessor(input_signal=audio, length=audio_len)\n",
        "    ```\n",
        "2. and groundtruth alignment (`attn_prior`).\n",
        "    ```python\n",
        "    attn_prior = torch.from_numpy(\n",
        "          beta_binomial_interpolator(spect_len.item(), text_len.item())\n",
        "        ).unsqueeze(0).to(text.device)\n",
        "    ```\n",
        "3. We use both of them to generate synthetic mel spectrogram via `spec_model.forward` method:\n",
        "    ```python\n",
        "    spectrogram = spec_model.forward(\n",
        "          text=text,\n",
        "          input_lens=text_len,\n",
        "          spec=spect,\n",
        "          mel_lens=spect_len,\n",
        "          attn_prior=attn_prior,\n",
        "          speaker=speaker,\n",
        "        )[0]\n",
        "    ```\n",
        "\n",
        "Repeat the above script for train and validation datasets as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b0918cc",
      "metadata": {
        "id": "9b0918cc"
      },
      "outputs": [],
      "source": [
        "# Example HiFi-GAN manifest:\n",
        "!head -n1 NeMoGermanTTS/train_manifest_text_normed_mel.json | jq"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "61cf4696",
      "metadata": {
        "id": "61cf4696"
      },
      "source": [
        "## Launch finetuning\n",
        "\n",
        "We will be re-using the existing HiFi-GAN config and HiFi-GAN pretrained on English."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "712df66f",
      "metadata": {
        "id": "712df66f"
      },
      "outputs": [],
      "source": [
        "!cd NeMoGermanTTS && CUDA_VISIBLE_DEVICES=0 python hifigan_finetune.py --config-path . --config-name hifigan.yaml \\\n",
        "    model.max_steps=10 \\\n",
        "    model.optim.lr=0.00001 \\\n",
        "    ~model.optim.sched \\\n",
        "    train_dataset=train_manifest_text_normed_mel.json \\\n",
        "    validation_datasets=val_manifest_text_normed_mel.json \\\n",
        "    exp_manager.exp_dir=resultGermanTTS \\\n",
        "    +init_from_pretrained_model={hfg_ngc} \\\n",
        "    +trainer.val_check_interval=5 \\\n",
        "    trainer.check_val_every_n_epoch=null \\\n",
        "    model/train_ds=train_ds_finetune \\\n",
        "    model/validation_ds=val_ds_finetune \\\n",
        "    exp_manager.create_wandb_logger=true \\\n",
        "    exp_manager.wandb_logger_kwargs.name=\"tutorial_2\" \\\n",
        "    exp_manager.wandb_logger_kwargs.project=\"GermanTTS\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "f1e18615",
      "metadata": {
        "id": "f1e18615"
      },
      "source": [
        "<b>Note</b>: We've limited the above run to 10 steps only, so we can validate the implementation within the scope of this tutorial. We recommend evaluating around every 50 steps HiFi-GAN until you get desired quality results."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "bb4a8c7b",
      "metadata": {
        "id": "bb4a8c7b"
      },
      "source": [
        "## Evaluating FastPitch and Finetuned HiFi-GAN\n",
        "\n",
        "Let's evaluate the quality of the FastPitch model generated so far using a HiFi-GAN model finetuned on predicted mels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd683d45",
      "metadata": {
        "id": "bd683d45"
      },
      "outputs": [],
      "source": [
        "hfg_path = sorted(glob.glob(\"NeMoGermanTTS/resultGermanTTS/HifiGan/*/checkpoints/HifiGan.nemo\"), key=os.path.getmtime)[-1]\n",
        "\n",
        "if \".nemo\" in hfg_path:\n",
        "    vocoder_model_pt = HifiGanModel.restore_from(hfg_path).eval().cuda()\n",
        "else:\n",
        "    vocoder_model_pt = HifiGanModel.load_from_checkpoint(checkpoint_path=hfg_path).eval().cuda()\n",
        "\n",
        "if \".nemo\" in fastpitch_model_path:\n",
        "    spec_gen_model = FastPitchModel.restore_from(fastpitch_model_path).eval().cuda()\n",
        "else:\n",
        "    spec_gen_model = FastPitchModel.load_from_checkpoint(checkpoint_path=fastpitch_model_path).eval().cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a7b7aa2",
      "metadata": {
        "id": "0a7b7aa2"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "audio, spectrogram = evaluate_spec_fastpitch_ckpt(spec_gen_model, vocoder_model_pt, test)\n",
        "\n",
        "# visualize the spectrogram\n",
        "if spectrogram is not None:\n",
        "    imshow(spectrogram, origin=\"lower\")\n",
        "    plt.show()\n",
        "\n",
        "# audio\n",
        "print(\"original audio\")\n",
        "ipd.display(ipd.Audio(data_path+test_id+'.wav', rate=22050))\n",
        "print(\"predicted audio\")\n",
        "ipd.display(ipd.Audio(audio, rate=22050))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "0700e6f4",
      "metadata": {
        "id": "0700e6f4"
      },
      "source": [
        "That's it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UzfwExWZ-xY4",
      "metadata": {
        "id": "UzfwExWZ-xY4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

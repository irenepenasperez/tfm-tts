{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENTO COMPLETO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero de todo, se deben realizar la Extracción, Transformación y Carga (ETL) de los datos.\n",
    "\n",
    "El cuaderno desde el que se realiza es: FP_ES_TTS_ExtraccionTransformacionLimpiezaDataset.ipynb\n",
    "Una vez ejecutado dicho cuaderno, tenemos en la carpeta correspondiente (en este caso en la carpeta datasets/datasetCastellanoReducido) los tres ficheros creados por el cuaderno, llamados metadata_dev, metadata_test y metadata_train.\n",
    "\n",
    "A continuación, con esos ficheros tenemos que hacer los siguientes pasos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creación de los Manifiestos de Datos.\n",
    "\n",
    "Se ha creado un script llamado `scripts/dataset_processing/tts/thorsten_neutral/get_data.py`, para generar las divisiones train/val/test en formato de manifiesto JSON JSON con los siguientes campos:\n",
    "1. `audio_filepath`: localización del archivo de audio (wav);\n",
    "2. `duration`: duración del archivo de audio (wav);\n",
    "3. `text`: texto original;\n",
    "4. `normalized_text`: texto normalizado a través del pipeline de normalización.\n",
    "\n",
    "Una vez ejecutado este comando, se obtienen los manifiestos finales `train_manifest_text_normed.json`, `val_manifest_text_normed.json` y `test_manifest_text_normed.json`.\n",
    "El script puede tardar en ejecutarse más de 1 hora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset directory found\n",
      "[NeMo I 2023-09-11 17:15:40 es_get_data:148] Preparing JSON train/val/test splits.\n",
      "130it [00:05, 25.95it/s]^C\n",
      "130it [00:05, 24.32it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/irene/notebooks/es_get_data.py\", line 277, in <module>\n",
      "    main()\n",
      "  File \"/home/irene/notebooks/es_get_data.py\", line 235, in main\n",
      "    entries_train, entries_val, entries_test, not_found_wavs, wrong_duration_wavs = __process_data(\n",
      "  File \"/home/irene/notebooks/es_get_data.py\", line 175, in __process_data\n",
      "    duration = subprocess.check_output(f\"soxi -D {wav_file}\", shell=True)\n",
      "  File \"/usr/lib/python3.10/subprocess.py\", line 420, in check_output\n",
      "    return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n",
      "  File \"/usr/lib/python3.10/subprocess.py\", line 503, in run\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/home/irene/notebooks\")\n",
    "\n",
    "!python es_get_data.py \\\n",
    "    --data-root /home/irene/datasets \\\n",
    "    --manifests-root ../datasets/NemoSpanishTTSEsMapa152Finetuning \\\n",
    "    --val-size 3 \\\n",
    "    --test-size 2 \\\n",
    "    --seed-for-ds-split 87 \\\n",
    "    --num-workers -1 \\\n",
    "    --normalize-text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paso necesario para los datasets que se hayan tenido que convertir a wav.\n",
    "\n",
    "A continuación se hace un paso necesario para los datasets del profesor de Segovia, debido a que, al principio estaban en un formato diferente. Después, mediante un script se convirtieron de mp4 a wav https://colab.research.google.com/drive/14izC7G5e-R2LlxvdRG8oCUH25QwZmlmH#scrollTo=OXVrAcJ0ebrQ. Pero faltaba por cambiar el canal, ya que estaba en estereo. Se cambia a mono, y de esta manera, funcionan los siguientes pasos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esto soluciona un error de canales: así si que se extrae la información suplementaria: se pasa de estereo a mono.\n",
    "from pydub import AudioSegment\n",
    "import os\n",
    "\n",
    "folder_path = \"/home/irene/datasets/datasetEsMapa152Finetuning/esmapa152\"\n",
    "\n",
    "# Iterate over the files in the folder\n",
    "# for i in range(92):\n",
    "    # file_name = f\"esmapa{i}.wav\" # Formato normal datasetBueno\n",
    "for i in range(153):\n",
    "    file_name = f\"esmapa{i:04d}.wav\"  # Formato con varios 0 DatasetEsMapa\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "    # Check if the file exists before processing\n",
    "    if os.path.exists(file_path):\n",
    "        # Load the audio file\n",
    "        sound = AudioSegment.from_wav(file_path)\n",
    "\n",
    "        # Set the number of channels to 1\n",
    "        sound = sound.set_channels(1)\n",
    "\n",
    "        # Export the modified audio back to the same file path\n",
    "        sound.export(file_path, format=\"wav\")\n",
    "\n",
    "        print(f\"Converted file: {file_name}\")\n",
    "    else:\n",
    "        print(f\"File not found: {file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extracción de los datos suplementarios\n",
    "\n",
    "Para acelerar y estabilizar el entrenamiento, se necesitan extraer datos suplementarios para cada audio, estimando las estadísticas de tono (media, desviación estándar, mínimo y máximo). Para realizar esto, se necesita iterar sobre los datos una vez, a través del script `extract_sup_data.py`.\n",
    "Se han retocado algunos parámetros del fichero de configuración: /home/irene/datasets/NemoSpanishTTS/ds_for_fastpitch_align.yaml\n",
    "\n",
    "**Nota**: Este es un paso opcional, pero se ha realizado al crear el modelo base.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os\n",
    "os.chdir(\"/home/irene/datasets/NemoSpanishTTSEsMapa152Finetuning\")\n",
    "\n",
    "!python extract_sup_data.py \\\n",
    "        --config-path ./ \\\n",
    "        --config-name ds_for_fastpitch_align.yaml \\\n",
    "        manifest_filepath=train_manifest_text_normed.json \\\n",
    "        sup_data_path=sup_data \\\n",
    "        ++dataloader_params.num_workers=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Entrenamiento\n",
    "\n",
    "Antes de entrenar el modelo, hay que definir la configuración del mismo. Se han cambiado algunas cosas con respecto al modelo original\n",
    " `examples/tts/conf/de/fastpitch_align_22050_grapheme.yaml`.\n",
    " \n",
    " Por otro lado, los valores de `pitch_mean` y `pitch_std` deben ser actualziados con los valores que se han estimado en el paso de`extract_sup_data.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ha usado Wandb para tener los resultados de los experimentos [enlace](https://docs.wandb.ai/ref/cli/wandb-login)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    }
   ],
   "source": [
    "!wandb login 7f8717dd64209b51a51493f579c375a7ca34fd2f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora se va a proceder a entrenar el modelo, en este caso FastPitch.En el siguiente comando se van a poner los valores de `PITCH_MEAN` y `PITCH_STD`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!(cd /home/irene/datasets/NemoSpanishTTS && CUDA_VISIBLE_DEVICES=0 python fastpitch.py --config-path . --config-name fastpitch_align_44100 \\\n",
    "  model.train_ds.dataloader_params.batch_size=32 \\\n",
    "    model.validation_ds.dataloader_params.batch_size=32 \\\n",
    "    train_dataset=train_manifest_text_normed.json \\\n",
    "    validation_datasets=val_manifest_text_normed.json \\\n",
    "    sup_data_path=sup_data \\\n",
    "    exp_manager.exp_dir=resultSpanishTTS \\\n",
    "    trainer.check_val_every_n_epoch=1 \\\n",
    "    pitch_mean=126.73465728759766 \\\n",
    "    pitch_std=38.099849700927734 \\\n",
    "    +exp_manager.create_wandb_logger=true \\\n",
    "    +exp_manager.wandb_logger_kwargs.name=\"tutorial\" \\\n",
    "    +exp_manager.wandb_logger_kwargs.project=\"SpanishTTS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota:\n",
    "1. Se usa `CUDA_VISIBLE_DEVICES=0` para limitar el entrenamiento a una sola GPU.\n",
    "2. Para hacer el debugging se utiliza el siguiente flag: `HYDRA_FULL_ERROR=1`, `CUDA_LAUNCH_BLOCKING=1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después del entreamiento, se procede a realizar la evaluación. Para ello se tiene el cuaderno llamado FP_ES_TTS_Evaluate.ipynb. En este cuaderno, está tanto la evaluación y la creación de los audios del modelo base como del finetuned de FastPitch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Finetuning FastPitch\n",
    "Mejora de la calidad del habla mediante el ajuste de FastPitch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para realizar el Finetuning, se poseen otros dos datasets. Una vez hecho el proceso de ETL, y obtención de los manifiestos y la extracción de los datos suplementarios, se procede a la realización del finetuning. Éste se encuentra en el script  fptts-finetuningFastPitch.sh.\n",
    "La evaluación del modelo se puede realizar en el cuaderno FP_ES_TTS_Evaluate.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Finetuning HiFi-GAN\n",
    "Mejora de la calidad del habla mediante el ajuste de HiFi-GAN en mel-espectrogramas sintetizados de FastPitch.\n",
    " \n",
    "Se ha realizado el siguiente cuaderno: FP_ES_TTS_Finetuning_HiFiGAN.ipynb.\n",
    "\n",
    "Para evaluar los audios resultantes, se ha creado el cuaderno FP_ES_TTS_Evaluate-FTHifiGAN.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: tts-scores in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tts-scores) (4.65.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from tts-scores) (1.10.1)\n",
      "Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.10/dist-packages (from tts-scores) (2.0.1)\n",
      "Requirement already satisfied: torchaudio>0.9 in /usr/local/lib/python3.10/dist-packages (from tts-scores) (2.0.2)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from tts-scores) (4.30.2)\n",
      "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (from tts-scores) (0.13.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from tts-scores) (2.31.0)\n",
      "Requirement already satisfied: ffmpeg in /usr/local/lib/python3.10/dist-packages (from tts-scores) (1.4)\n",
      "Requirement already satisfied: unidecode in /usr/local/lib/python3.10/dist-packages (from tts-scores) (1.3.6)\n",
      "Requirement already satisfied: inflect in /usr/local/lib/python3.10/dist-packages (from tts-scores) (6.0.4)\n",
      "Requirement already satisfied: pytorch-fid in /usr/local/lib/python3.10/dist-packages (from tts-scores) (0.3.0)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from tts-scores) (0.6.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->tts-scores) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->tts-scores) (4.6.3)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->tts-scores) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->tts-scores) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->tts-scores) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->tts-scores) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->tts-scores) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->tts-scores) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->tts-scores) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->tts-scores) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->tts-scores) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->tts-scores) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->tts-scores) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->tts-scores) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->tts-scores) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->tts-scores) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->tts-scores) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.8->tts-scores) (65.5.1)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.8->tts-scores) (0.40.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8->tts-scores) (3.24.1.1)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8->tts-scores) (16.0.6)\n",
      "Requirement already satisfied: pydantic>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from inflect->tts-scores) (1.10.9)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pytorch-fid->tts-scores) (1.22.2)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pytorch-fid->tts-scores) (9.2.0)\n",
      "Requirement already satisfied: torchvision>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-fid->tts-scores) (0.15.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->tts-scores) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->tts-scores) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->tts-scores) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->tts-scores) (2023.5.7)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers->tts-scores) (0.15.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers->tts-scores) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->tts-scores) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->tts-scores) (2023.6.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers->tts-scores) (0.3.1)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers->tts-scores) (2023.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8->tts-scores) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8->tts-scores) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tts-scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MEJORES MODELOS\n",
    "Se van a elegir los mejores modelos, para después realizar una evaluación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los mejores modelos son:\n",
    "- Modelo base FastPitch: '/home/irene/datasets/NemoSpanishTTS/resultSpanishTTS/FastPitch/2023-06-28_18-47-07/checkpoints/FastPitch--val_loss=0.7140-epoch=265.ckpt'\n",
    "- Modelo base HiFiGAN: -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MÉTRICAS CLVP \n",
    "En un principio se encontraron las métricas CLVP. Sin embargo, cuando se inicializa la métrica CLVP, salta un error: \n",
    "\n",
    "RuntimeError: Error(s) in loading state_dict for CLVP:\n",
    "   Missing key(s) in state_dict: \"text_pos_emb.weight\", \"text_transformer.layers.layers.0.0.scale\" ....\n",
    "   .....  mismatch for to_speech_latent.weight: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
    "\n",
    "   Se ha buscado en Internet y se vió un issue abierto en github, en donde se comentaba que todavía no se había encontrado solución:\n",
    "   https://github.com/neonbjb/tts-scores/issues/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tts_scores.clvp import CLVPMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_checkpoint = '/home/irene/notebooks/data/clvp.pth' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for CLVP:\n\tMissing key(s) in state_dict: \"text_pos_emb.weight\", \"text_transformer.layers.layers.0.0.scale\", \"text_transformer.layers.layers.0.0.fn.norm.weight\", \"text_transformer.layers.layers.0.0.fn.norm.bias\", \"text_transformer.layers.layers.0.0.fn.fn.to_qkv.weight\", \"text_transformer.layers.layers.0.0.fn.fn.to_out.0.weight\", \"text_transformer.layers.layers.0.0.fn.fn.to_out.0.bias\", \"text_transformer.layers.layers.0.1.scale\", \"text_transformer.layers.layers.0.1.fn.norm.weight\", \"text_transformer.layers.layers.0.1.fn.norm.bias\", \"text_transformer.layers.layers.0.1.fn.fn.net.0.weight\", \"text_transformer.layers.layers.0.1.fn.fn.net.0.bias\", \"text_transformer.layers.layers.0.1.fn.fn.net.3.weight\", \"text_transformer.layers.layers.0.1.fn.fn.net.3.bias\", \"text_transformer.layers.layers.1.0.scale\", \"text_transformer.layers.layers.1.0.fn.norm.weight\", \"text_transformer.layers.layers.1.0.fn.norm.bias\", \"text_transformer.layers.layers.1.0.fn.fn.to_qkv.weight\", \"text_transformer.layers.layers.1.0.fn.fn.to_out.0.weight\", \"text_transformer.layers.layers.1.0.fn.fn.to_out.0.bias\", \"text_transformer.layers.layers.1.1.scale\", \"text_transformer.layers.layers.1.1.fn.norm.weight\", \"text_transformer.layers.layers.1.1.fn.norm.bias\", \"text_transformer.layers.layers.1.1.fn.fn.net.0.weight\", \"text_transformer.layers.layers.1.1.fn.fn.net.0.bias\", \"text_transformer.layers.layers.1.1.fn.fn.net.3.weight\", \"text_transformer.layers.layers.1.1.fn.fn.net.3.bias\", \"text_transformer.layers.layers.2.0.scale\", \"text_transformer.layers.layers.2.0.fn.norm.weight\", \"text_transformer.layers.layers.2.0.fn.norm.bias\", \"text_transformer.layers.layers.2.0.fn.fn.to_qkv.weight\", \"text_transformer.layers.layers.2.0.fn.fn.to_out.0.weight\", \"text_transformer.layers.layers.2.0.fn.fn.to_out.0.bias\", \"text_transformer.layers.layers.2.1.scale\", \"text_transformer.layers.layers.2.1.fn.norm.weight\", \"text_transformer.layers.layers.2.1.fn.norm.bias\", \"text_transformer.layers.layers.2.1.fn.fn.net.0.weight\", \"text_transformer.layers.layers.2.1.fn.fn.net.0.bias\", \"text_transformer.layers.layers.2.1.fn.fn.net.3.weight\", \"text_transformer.layers.layers.2.1.fn.fn.net.3.bias\", \"text_transformer.layers.layers.3.0.scale\", \"text_transformer.layers.layers.3.0.fn.norm.weight\", \"text_transformer.layers.layers.3.0.fn.norm.bias\", \"text_transformer.layers.layers.3.0.fn.fn.to_qkv.weight\", \"text_transformer.layers.layers.3.0.fn.fn.to_out.0.weight\", \"text_transformer.layers.layers.3.0.fn.fn.to_out.0.bias\", \"text_transformer.layers.layers.3.1.scale\", \"text_transformer.layers.layers.3.1.fn.norm.weight\", \"text_transformer.layers.layers.3.1.fn.norm.bias\", \"text_transformer.layers.layers.3.1.fn.fn.net.0.weight\", \"text_transformer.layers.layers.3.1.fn.fn.net.0.bias\", \"text_transformer.layers.layers.3.1.fn.fn.net.3.weight\", \"text_transformer.layers.layers.3.1.fn.fn.net.3.bias\", \"text_transformer.layers.layers.4.0.scale\", \"text_transformer.layers.layers.4.0.fn.norm.weight\", \"text_transformer.layers.layers.4.0.fn.norm.bias\", \"text_transformer.layers.layers.4.0.fn.fn.to_qkv.weight\", \"text_transformer.layers.layers.4.0.fn.fn.to_out.0.weight\", \"text_transformer.layers.layers.4.0.fn.fn.to_out.0.bias\", \"text_transformer.layers.layers.4.1.scale\", \"text_transformer.layers.layers.4.1.fn.norm.weight\", \"text_transformer.layers.layers.4.1.fn.norm.bias\", \"text_transformer.layers.layers.4.1.fn.fn.net.0.weight\", \"text_transformer.layers.layers.4.1.fn.fn.net.0.bias\", \"text_transformer.layers.layers.4.1.fn.fn.net.3.weight\", \"text_transformer.layers.layers.4.1.fn.fn.net.3.bias\", \"text_transformer.layers.layers.5.0.scale\", \"text_transformer.layers.layers.5.0.fn.norm.weight\", \"text_transformer.layers.layers.5.0.fn.norm.bias\", \"text_transformer.layers.layers.5.0.fn.fn.to_qkv.weight\", \"text_transformer.layers.layers.5.0.fn.fn.to_out.0.weight\", \"text_transformer.layers.layers.5.0.fn.fn.to_out.0.bias\", \"text_transformer.layers.layers.5.1.scale\", \"text_transformer.layers.layers.5.1.fn.norm.weight\", \"text_transformer.layers.layers.5.1.fn.norm.bias\", \"text_transformer.layers.layers.5.1.fn.fn.net.0.weight\", \"text_transformer.layers.layers.5.1.fn.fn.net.0.bias\", \"text_transformer.layers.layers.5.1.fn.fn.net.3.weight\", \"text_transformer.layers.layers.5.1.fn.fn.net.3.bias\", \"text_transformer.layers.layers.6.0.scale\", \"text_transformer.layers.layers.6.0.fn.norm.weight\", \"text_transformer.layers.layers.6.0.fn.norm.bias\", \"text_transformer.layers.layers.6.0.fn.fn.to_qkv.weight\", \"text_transformer.layers.layers.6.0.fn.fn.to_out.0.weight\", \"text_transformer.layers.layers.6.0.fn.fn.to_out.0.bias\", \"text_transformer.layers.layers.6.1.scale\", \"text_transformer.layers.layers.6.1.fn.norm.weight\", \"text_transformer.layers.layers.6.1.fn.norm.bias\", \"text_transformer.layers.layers.6.1.fn.fn.net.0.weight\", \"text_transformer.layers.layers.6.1.fn.fn.net.0.bias\", \"text_transformer.layers.layers.6.1.fn.fn.net.3.weight\", \"text_transformer.layers.layers.6.1.fn.fn.net.3.bias\", \"text_transformer.layers.layers.7.0.scale\", \"text_transformer.layers.layers.7.0.fn.norm.weight\", \"text_transformer.layers.layers.7.0.fn.norm.bias\", \"text_transformer.layers.layers.7.0.fn.fn.to_qkv.weight\", \"text_transformer.layers.layers.7.0.fn.fn.to_out.0.weight\", \"text_transformer.layers.layers.7.0.fn.fn.to_out.0.bias\", \"text_transformer.layers.layers.7.1.scale\", \"text_transformer.layers.layers.7.1.fn.norm.weight\", \"text_transformer.layers.layers.7.1.fn.norm.bias\", \"text_transformer.layers.layers.7.1.fn.fn.net.0.weight\", \"text_transformer.layers.layers.7.1.fn.fn.net.0.bias\", \"text_transformer.layers.layers.7.1.fn.fn.net.3.weight\", \"text_transformer.layers.layers.7.1.fn.fn.net.3.bias\", \"speech_enc.weight\", \"speech_enc.bias\", \"speech_pos_emb.weight\", \"speech_transformer.layers.layers.0.0.scale\", \"speech_transformer.layers.layers.0.0.fn.norm.weight\", \"speech_transformer.layers.layers.0.0.fn.norm.bias\", \"speech_transformer.layers.layers.0.0.fn.fn.to_qkv.weight\", \"speech_transformer.layers.layers.0.0.fn.fn.to_out.0.weight\", \"speech_transformer.layers.layers.0.0.fn.fn.to_out.0.bias\", \"speech_transformer.layers.layers.0.1.scale\", \"speech_transformer.layers.layers.0.1.fn.norm.weight\", \"speech_transformer.layers.layers.0.1.fn.norm.bias\", \"speech_transformer.layers.layers.0.1.fn.fn.net.0.weight\", \"speech_transformer.layers.layers.0.1.fn.fn.net.0.bias\", \"speech_transformer.layers.layers.0.1.fn.fn.net.3.weight\", \"speech_transformer.layers.layers.0.1.fn.fn.net.3.bias\", \"speech_transformer.layers.layers.1.0.scale\", \"speech_transformer.layers.layers.1.0.fn.norm.weight\", \"speech_transformer.layers.layers.1.0.fn.norm.bias\", \"speech_transformer.layers.layers.1.0.fn.fn.to_qkv.weight\", \"speech_transformer.layers.layers.1.0.fn.fn.to_out.0.weight\", \"speech_transformer.layers.layers.1.0.fn.fn.to_out.0.bias\", \"speech_transformer.layers.layers.1.1.scale\", \"speech_transformer.layers.layers.1.1.fn.norm.weight\", \"speech_transformer.layers.layers.1.1.fn.norm.bias\", \"speech_transformer.layers.layers.1.1.fn.fn.net.0.weight\", \"speech_transformer.layers.layers.1.1.fn.fn.net.0.bias\", \"speech_transformer.layers.layers.1.1.fn.fn.net.3.weight\", \"speech_transformer.layers.layers.1.1.fn.fn.net.3.bias\", \"speech_transformer.layers.layers.2.0.scale\", \"speech_transformer.layers.layers.2.0.fn.norm.weight\", \"speech_transformer.layers.layers.2.0.fn.norm.bias\", \"speech_transformer.layers.layers.2.0.fn.fn.to_qkv.weight\", \"speech_transformer.layers.layers.2.0.fn.fn.to_out.0.weight\", \"speech_transformer.layers.layers.2.0.fn.fn.to_out.0.bias\", \"speech_transformer.layers.layers.2.1.scale\", \"speech_transformer.layers.layers.2.1.fn.norm.weight\", \"speech_transformer.layers.layers.2.1.fn.norm.bias\", \"speech_transformer.layers.layers.2.1.fn.fn.net.0.weight\", \"speech_transformer.layers.layers.2.1.fn.fn.net.0.bias\", \"speech_transformer.layers.layers.2.1.fn.fn.net.3.weight\", \"speech_transformer.layers.layers.2.1.fn.fn.net.3.bias\", \"speech_transformer.layers.layers.3.0.scale\", \"speech_transformer.layers.layers.3.0.fn.norm.weight\", \"speech_transformer.layers.layers.3.0.fn.norm.bias\", \"speech_transformer.layers.layers.3.0.fn.fn.to_qkv.weight\", \"speech_transformer.layers.layers.3.0.fn.fn.to_out.0.weight\", \"speech_transformer.layers.layers.3.0.fn.fn.to_out.0.bias\", \"speech_transformer.layers.layers.3.1.scale\", \"speech_transformer.layers.layers.3.1.fn.norm.weight\", \"speech_transformer.layers.layers.3.1.fn.norm.bias\", \"speech_transformer.layers.layers.3.1.fn.fn.net.0.weight\", \"speech_transformer.layers.layers.3.1.fn.fn.net.0.bias\", \"speech_transformer.layers.layers.3.1.fn.fn.net.3.weight\", \"speech_transformer.layers.layers.3.1.fn.fn.net.3.bias\", \"speech_transformer.layers.layers.4.0.scale\", \"speech_transformer.layers.layers.4.0.fn.norm.weight\", \"speech_transformer.layers.layers.4.0.fn.norm.bias\", \"speech_transformer.layers.layers.4.0.fn.fn.to_qkv.weight\", \"speech_transformer.layers.layers.4.0.fn.fn.to_out.0.weight\", \"speech_transformer.layers.layers.4.0.fn.fn.to_out.0.bias\", \"speech_transformer.layers.layers.4.1.scale\", \"speech_transformer.layers.layers.4.1.fn.norm.weight\", \"speech_transformer.layers.layers.4.1.fn.norm.bias\", \"speech_transformer.layers.layers.4.1.fn.fn.net.0.weight\", \"speech_transformer.layers.layers.4.1.fn.fn.net.0.bias\", \"speech_transformer.layers.layers.4.1.fn.fn.net.3.weight\", \"speech_transformer.layers.layers.4.1.fn.fn.net.3.bias\", \"speech_transformer.layers.layers.5.0.scale\", \"speech_transformer.layers.layers.5.0.fn.norm.weight\", \"speech_transformer.layers.layers.5.0.fn.norm.bias\", \"speech_transformer.layers.layers.5.0.fn.fn.to_qkv.weight\", \"speech_transformer.layers.layers.5.0.fn.fn.to_out.0.weight\", \"speech_transformer.layers.layers.5.0.fn.fn.to_out.0.bias\", \"speech_transformer.layers.layers.5.1.scale\", \"speech_transformer.layers.layers.5.1.fn.norm.weight\", \"speech_transformer.layers.layers.5.1.fn.norm.bias\", \"speech_transformer.layers.layers.5.1.fn.fn.net.0.weight\", \"speech_transformer.layers.layers.5.1.fn.fn.net.0.bias\", \"speech_transformer.layers.layers.5.1.fn.fn.net.3.weight\", \"speech_transformer.layers.layers.5.1.fn.fn.net.3.bias\", \"speech_transformer.layers.layers.6.0.scale\", \"speech_transformer.layers.layers.6.0.fn.norm.weight\", \"speech_transformer.layers.layers.6.0.fn.norm.bias\", \"speech_transformer.layers.layers.6.0.fn.fn.to_qkv.weight\", \"speech_transformer.layers.layers.6.0.fn.fn.to_out.0.weight\", \"speech_transformer.layers.layers.6.0.fn.fn.to_out.0.bias\", \"speech_transformer.layers.layers.6.1.scale\", \"speech_transformer.layers.layers.6.1.fn.norm.weight\", \"speech_transformer.layers.layers.6.1.fn.norm.bias\", \"speech_transformer.layers.layers.6.1.fn.fn.net.0.weight\", \"speech_transformer.layers.layers.6.1.fn.fn.net.0.bias\", \"speech_transformer.layers.layers.6.1.fn.fn.net.3.weight\", \"speech_transformer.layers.layers.6.1.fn.fn.net.3.bias\", \"speech_transformer.layers.layers.7.0.scale\", \"speech_transformer.layers.layers.7.0.fn.norm.weight\", \"speech_transformer.layers.layers.7.0.fn.norm.bias\", \"speech_transformer.layers.layers.7.0.fn.fn.to_qkv.weight\", \"speech_transformer.layers.layers.7.0.fn.fn.to_out.0.weight\", \"speech_transformer.layers.layers.7.0.fn.fn.to_out.0.bias\", \"speech_transformer.layers.layers.7.1.scale\", \"speech_transformer.layers.layers.7.1.fn.norm.weight\", \"speech_transformer.layers.layers.7.1.fn.norm.bias\", \"speech_transformer.layers.layers.7.1.fn.fn.net.0.weight\", \"speech_transformer.layers.layers.7.1.fn.fn.net.0.bias\", \"speech_transformer.layers.layers.7.1.fn.fn.net.3.weight\", \"speech_transformer.layers.layers.7.1.fn.fn.net.3.bias\", \"speech_transformer.layers.layers.8.0.scale\", \"speech_transformer.layers.layers.8.0.fn.norm.weight\", \"speech_transformer.layers.layers.8.0.fn.norm.bias\", \"speech_transformer.layers.layers.8.0.fn.fn.to_qkv.weight\", \"speech_transformer.layers.layers.8.0.fn.fn.to_out.0.weight\", \"speech_transformer.layers.layers.8.0.fn.fn.to_out.0.bias\", \"speech_transformer.layers.layers.8.1.scale\", \"speech_transformer.layers.layers.8.1.fn.norm.weight\", \"speech_transformer.layers.layers.8.1.fn.norm.bias\", \"speech_transformer.layers.layers.8.1.fn.fn.net.0.weight\", \"speech_transformer.layers.layers.8.1.fn.fn.net.0.bias\", \"speech_transformer.layers.layers.8.1.fn.fn.net.3.weight\", \"speech_transformer.layers.layers.8.1.fn.fn.net.3.bias\", \"speech_transformer.layers.layers.9.0.scale\", \"speech_transformer.layers.layers.9.0.fn.norm.weight\", \"speech_transformer.layers.layers.9.0.fn.norm.bias\", \"speech_transformer.layers.layers.9.0.fn.fn.to_qkv.weight\", \"speech_transformer.layers.layers.9.0.fn.fn.to_out.0.weight\", \"speech_transformer.layers.layers.9.0.fn.fn.to_out.0.bias\", \"speech_transformer.layers.layers.9.1.scale\", \"speech_transformer.layers.layers.9.1.fn.norm.weight\", \"speech_transformer.layers.layers.9.1.fn.norm.bias\", \"speech_transformer.layers.layers.9.1.fn.fn.net.0.weight\", \"speech_transformer.layers.layers.9.1.fn.fn.net.0.bias\", \"speech_transformer.layers.layers.9.1.fn.fn.net.3.weight\", \"speech_transformer.layers.layers.9.1.fn.fn.net.3.bias\". \n\tUnexpected key(s) in state_dict: \"speech_emb.weight\", \"text_transformer.transformer.attn_layers.layers.0.0.0.g\", \"text_transformer.transformer.attn_layers.layers.0.1.wrap.to_q.weight\", \"text_transformer.transformer.attn_layers.layers.0.1.wrap.to_k.weight\", \"text_transformer.transformer.attn_layers.layers.0.1.wrap.to_v.weight\", \"text_transformer.transformer.attn_layers.layers.0.1.wrap.to_out.weight\", \"text_transformer.transformer.attn_layers.layers.0.1.wrap.to_out.bias\", \"text_transformer.transformer.attn_layers.layers.1.0.0.g\", \"text_transformer.transformer.attn_layers.layers.1.1.wrap.net.0.proj.weight\", \"text_transformer.transformer.attn_layers.layers.1.1.wrap.net.0.proj.bias\", \"text_transformer.transformer.attn_layers.layers.1.1.wrap.net.3.weight\", \"text_transformer.transformer.attn_layers.layers.1.1.wrap.net.3.bias\", \"text_transformer.transformer.attn_layers.layers.2.0.0.g\", \"text_transformer.transformer.attn_layers.layers.2.1.wrap.to_q.weight\", \"text_transformer.transformer.attn_layers.layers.2.1.wrap.to_k.weight\", \"text_transformer.transformer.attn_layers.layers.2.1.wrap.to_v.weight\", \"text_transformer.transformer.attn_layers.layers.2.1.wrap.to_out.weight\", \"text_transformer.transformer.attn_layers.layers.2.1.wrap.to_out.bias\", \"text_transformer.transformer.attn_layers.layers.3.0.0.g\", \"text_transformer.transformer.attn_layers.layers.3.1.wrap.net.0.proj.weight\", \"text_transformer.transformer.attn_layers.layers.3.1.wrap.net.0.proj.bias\", \"text_transformer.transformer.attn_layers.layers.3.1.wrap.net.3.weight\", \"text_transformer.transformer.attn_layers.layers.3.1.wrap.net.3.bias\", \"text_transformer.transformer.attn_layers.layers.4.0.0.g\", \"text_transformer.transformer.attn_layers.layers.4.1.wrap.to_q.weight\", \"text_transformer.transformer.attn_layers.layers.4.1.wrap.to_k.weight\", \"text_transformer.transformer.attn_layers.layers.4.1.wrap.to_v.weight\", \"text_transformer.transformer.attn_layers.layers.4.1.wrap.to_out.weight\", \"text_transformer.transformer.attn_layers.layers.4.1.wrap.to_out.bias\", \"text_transformer.transformer.attn_layers.layers.5.0.0.g\", \"text_transformer.transformer.attn_layers.layers.5.1.wrap.net.0.proj.weight\", \"text_transformer.transformer.attn_layers.layers.5.1.wrap.net.0.proj.bias\", \"text_transformer.transformer.attn_layers.layers.5.1.wrap.net.3.weight\", \"text_transformer.transformer.attn_layers.layers.5.1.wrap.net.3.bias\", \"text_transformer.transformer.attn_layers.layers.6.0.0.g\", \"text_transformer.transformer.attn_layers.layers.6.1.wrap.to_q.weight\", \"text_transformer.transformer.attn_layers.layers.6.1.wrap.to_k.weight\", \"text_transformer.transformer.attn_layers.layers.6.1.wrap.to_v.weight\", \"text_transformer.transformer.attn_layers.layers.6.1.wrap.to_out.weight\", \"text_transformer.transformer.attn_layers.layers.6.1.wrap.to_out.bias\", \"text_transformer.transformer.attn_layers.layers.7.0.0.g\", \"text_transformer.transformer.attn_layers.layers.7.1.wrap.net.0.proj.weight\", \"text_transformer.transformer.attn_layers.layers.7.1.wrap.net.0.proj.bias\", \"text_transformer.transformer.attn_layers.layers.7.1.wrap.net.3.weight\", \"text_transformer.transformer.attn_layers.layers.7.1.wrap.net.3.bias\", \"text_transformer.transformer.attn_layers.layers.8.0.0.g\", \"text_transformer.transformer.attn_layers.layers.8.1.wrap.to_q.weight\", \"text_transformer.transformer.attn_layers.layers.8.1.wrap.to_k.weight\", \"text_transformer.transformer.attn_layers.layers.8.1.wrap.to_v.weight\", \"text_transformer.transformer.attn_layers.layers.8.1.wrap.to_out.weight\", \"text_transformer.transformer.attn_layers.layers.8.1.wrap.to_out.bias\", \"text_transformer.transformer.attn_layers.layers.9.0.0.g\", \"text_transformer.transformer.attn_layers.layers.9.1.wrap.net.0.proj.weight\", \"text_transformer.transformer.attn_layers.layers.9.1.wrap.net.0.proj.bias\", \"text_transformer.transformer.attn_layers.layers.9.1.wrap.net.3.weight\", \"text_transformer.transformer.attn_layers.layers.9.1.wrap.net.3.bias\", \"text_transformer.transformer.attn_layers.layers.10.0.0.g\", \"text_transformer.transformer.attn_layers.layers.10.1.wrap.to_q.weight\", \"text_transformer.transformer.attn_layers.layers.10.1.wrap.to_k.weight\", \"text_transformer.transformer.attn_layers.layers.10.1.wrap.to_v.weight\", \"text_transformer.transformer.attn_layers.layers.10.1.wrap.to_out.weight\", \"text_transformer.transformer.attn_layers.layers.10.1.wrap.to_out.bias\", \"text_transformer.transformer.attn_layers.layers.11.0.0.g\", \"text_transformer.transformer.attn_layers.layers.11.1.wrap.net.0.proj.weight\", \"text_transformer.transformer.attn_layers.layers.11.1.wrap.net.0.proj.bias\", \"text_transformer.transformer.attn_layers.layers.11.1.wrap.net.3.weight\", \"text_transformer.transformer.attn_layers.layers.11.1.wrap.net.3.bias\", \"text_transformer.transformer.attn_layers.layers.12.0.0.g\", \"text_transformer.transformer.attn_layers.layers.12.1.wrap.to_q.weight\", \"text_transformer.transformer.attn_layers.layers.12.1.wrap.to_k.weight\", \"text_transformer.transformer.attn_layers.layers.12.1.wrap.to_v.weight\", \"text_transformer.transformer.attn_layers.layers.12.1.wrap.to_out.weight\", \"text_transformer.transformer.attn_layers.layers.12.1.wrap.to_out.bias\", \"text_transformer.transformer.attn_layers.layers.13.0.0.g\", \"text_transformer.transformer.attn_layers.layers.13.1.wrap.net.0.proj.weight\", \"text_transformer.transformer.attn_layers.layers.13.1.wrap.net.0.proj.bias\", \"text_transformer.transformer.attn_layers.layers.13.1.wrap.net.3.weight\", \"text_transformer.transformer.attn_layers.layers.13.1.wrap.net.3.bias\", \"text_transformer.transformer.attn_layers.layers.14.0.0.g\", \"text_transformer.transformer.attn_layers.layers.14.1.wrap.to_q.weight\", \"text_transformer.transformer.attn_layers.layers.14.1.wrap.to_k.weight\", \"text_transformer.transformer.attn_layers.layers.14.1.wrap.to_v.weight\", \"text_transformer.transformer.attn_layers.layers.14.1.wrap.to_out.weight\", \"text_transformer.transformer.attn_layers.layers.14.1.wrap.to_out.bias\", \"text_transformer.transformer.attn_layers.layers.15.0.0.g\", \"text_transformer.transformer.attn_layers.layers.15.1.wrap.net.0.proj.weight\", \"text_transformer.transformer.attn_layers.layers.15.1.wrap.net.0.proj.bias\", \"text_transformer.transformer.attn_layers.layers.15.1.wrap.net.3.weight\", \"text_transformer.transformer.attn_layers.layers.15.1.wrap.net.3.bias\", \"text_transformer.transformer.attn_layers.layers.16.0.0.g\", \"text_transformer.transformer.attn_layers.layers.16.1.wrap.to_q.weight\", \"text_transformer.transformer.attn_layers.layers.16.1.wrap.to_k.weight\", \"text_transformer.transformer.attn_layers.layers.16.1.wrap.to_v.weight\", \"text_transformer.transformer.attn_layers.layers.16.1.wrap.to_out.weight\", \"text_transformer.transformer.attn_layers.layers.16.1.wrap.to_out.bias\", \"text_transformer.transformer.attn_layers.layers.17.0.0.g\", \"text_transformer.transformer.attn_layers.layers.17.1.wrap.net.0.proj.weight\", \"text_transformer.transformer.attn_layers.layers.17.1.wrap.net.0.proj.bias\", \"text_transformer.transformer.attn_layers.layers.17.1.wrap.net.3.weight\", \"text_transformer.transformer.attn_layers.layers.17.1.wrap.net.3.bias\", \"text_transformer.transformer.attn_layers.layers.18.0.0.g\", \"text_transformer.transformer.attn_layers.layers.18.1.wrap.to_q.weight\", \"text_transformer.transformer.attn_layers.layers.18.1.wrap.to_k.weight\", \"text_transformer.transformer.attn_layers.layers.18.1.wrap.to_v.weight\", \"text_transformer.transformer.attn_layers.layers.18.1.wrap.to_out.weight\", \"text_transformer.transformer.attn_layers.layers.18.1.wrap.to_out.bias\", \"text_transformer.transformer.attn_layers.layers.19.0.0.g\", \"text_transformer.transformer.attn_layers.layers.19.1.wrap.net.0.proj.weight\", \"text_transformer.transformer.attn_layers.layers.19.1.wrap.net.0.proj.bias\", \"text_transformer.transformer.attn_layers.layers.19.1.wrap.net.3.weight\", \"text_transformer.transformer.attn_layers.layers.19.1.wrap.net.3.bias\", \"text_transformer.transformer.attn_layers.layers.20.0.0.g\", \"text_transformer.transformer.attn_layers.layers.20.1.wrap.to_q.weight\", \"text_transformer.transformer.attn_layers.layers.20.1.wrap.to_k.weight\", \"text_transformer.transformer.attn_layers.layers.20.1.wrap.to_v.weight\", \"text_transformer.transformer.attn_layers.layers.20.1.wrap.to_out.weight\", \"text_transformer.transformer.attn_layers.layers.20.1.wrap.to_out.bias\", \"text_transformer.transformer.attn_layers.layers.21.0.0.g\", \"text_transformer.transformer.attn_layers.layers.21.1.wrap.net.0.proj.weight\", \"text_transformer.transformer.attn_layers.layers.21.1.wrap.net.0.proj.bias\", \"text_transformer.transformer.attn_layers.layers.21.1.wrap.net.3.weight\", \"text_transformer.transformer.attn_layers.layers.21.1.wrap.net.3.bias\", \"text_transformer.transformer.attn_layers.layers.22.0.0.g\", \"text_transformer.transformer.attn_layers.layers.22.1.wrap.to_q.weight\", \"text_transformer.transformer.attn_layers.layers.22.1.wrap.to_k.weight\", \"text_transformer.transformer.attn_layers.layers.22.1.wrap.to_v.weight\", \"text_transformer.transformer.attn_layers.layers.22.1.wrap.to_out.weight\", \"text_transformer.transformer.attn_layers.layers.22.1.wrap.to_out.bias\", \"text_transformer.transformer.attn_layers.layers.23.0.0.g\", \"text_transformer.transformer.attn_layers.layers.23.1.wrap.net.0.proj.weight\", \"text_transformer.transformer.attn_layers.layers.23.1.wrap.net.0.proj.bias\", \"text_transformer.transformer.attn_layers.layers.23.1.wrap.net.3.weight\", \"text_transformer.transformer.attn_layers.layers.23.1.wrap.net.3.bias\", \"text_transformer.transformer.attn_layers.rotary_pos_emb.inv_freq\", \"text_transformer.transformer.norm.weight\", \"text_transformer.transformer.norm.bias\", \"speech_transformer.transformer.attn_layers.layers.0.0.0.g\", \"speech_transformer.transformer.attn_layers.layers.0.1.wrap.to_q.weight\", \"speech_transformer.transformer.attn_layers.layers.0.1.wrap.to_k.weight\", \"speech_transformer.transformer.attn_layers.layers.0.1.wrap.to_v.weight\", \"speech_transformer.transformer.attn_layers.layers.0.1.wrap.to_out.weight\", \"speech_transformer.transformer.attn_layers.layers.0.1.wrap.to_out.bias\", \"speech_transformer.transformer.attn_layers.layers.1.0.0.g\", \"speech_transformer.transformer.attn_layers.layers.1.1.wrap.net.0.proj.weight\", \"speech_transformer.transformer.attn_layers.layers.1.1.wrap.net.0.proj.bias\", \"speech_transformer.transformer.attn_layers.layers.1.1.wrap.net.3.weight\", \"speech_transformer.transformer.attn_layers.layers.1.1.wrap.net.3.bias\", \"speech_transformer.transformer.attn_layers.layers.2.0.0.g\", \"speech_transformer.transformer.attn_layers.layers.2.1.wrap.to_q.weight\", \"speech_transformer.transformer.attn_layers.layers.2.1.wrap.to_k.weight\", \"speech_transformer.transformer.attn_layers.layers.2.1.wrap.to_v.weight\", \"speech_transformer.transformer.attn_layers.layers.2.1.wrap.to_out.weight\", \"speech_transformer.transformer.attn_layers.layers.2.1.wrap.to_out.bias\", \"speech_transformer.transformer.attn_layers.layers.3.0.0.g\", \"speech_transformer.transformer.attn_layers.layers.3.1.wrap.net.0.proj.weight\", \"speech_transformer.transformer.attn_layers.layers.3.1.wrap.net.0.proj.bias\", \"speech_transformer.transformer.attn_layers.layers.3.1.wrap.net.3.weight\", \"speech_transformer.transformer.attn_layers.layers.3.1.wrap.net.3.bias\", \"speech_transformer.transformer.attn_layers.layers.4.0.0.g\", \"speech_transformer.transformer.attn_layers.layers.4.1.wrap.to_q.weight\", \"speech_transformer.transformer.attn_layers.layers.4.1.wrap.to_k.weight\", \"speech_transformer.transformer.attn_layers.layers.4.1.wrap.to_v.weight\", \"speech_transformer.transformer.attn_layers.layers.4.1.wrap.to_out.weight\", \"speech_transformer.transformer.attn_layers.layers.4.1.wrap.to_out.bias\", \"speech_transformer.transformer.attn_layers.layers.5.0.0.g\", \"speech_transformer.transformer.attn_layers.layers.5.1.wrap.net.0.proj.weight\", \"speech_transformer.transformer.attn_layers.layers.5.1.wrap.net.0.proj.bias\", \"speech_transformer.transformer.attn_layers.layers.5.1.wrap.net.3.weight\", \"speech_transformer.transformer.attn_layers.layers.5.1.wrap.net.3.bias\", \"speech_transformer.transformer.attn_layers.layers.6.0.0.g\", \"speech_transformer.transformer.attn_layers.layers.6.1.wrap.to_q.weight\", \"speech_transformer.transformer.attn_layers.layers.6.1.wrap.to_k.weight\", \"speech_transformer.transformer.attn_layers.layers.6.1.wrap.to_v.weight\", \"speech_transformer.transformer.attn_layers.layers.6.1.wrap.to_out.weight\", \"speech_transformer.transformer.attn_layers.layers.6.1.wrap.to_out.bias\", \"speech_transformer.transformer.attn_layers.layers.7.0.0.g\", \"speech_transformer.transformer.attn_layers.layers.7.1.wrap.net.0.proj.weight\", \"speech_transformer.transformer.attn_layers.layers.7.1.wrap.net.0.proj.bias\", \"speech_transformer.transformer.attn_layers.layers.7.1.wrap.net.3.weight\", \"speech_transformer.transformer.attn_layers.layers.7.1.wrap.net.3.bias\", \"speech_transformer.transformer.attn_layers.layers.8.0.0.g\", \"speech_transformer.transformer.attn_layers.layers.8.1.wrap.to_q.weight\", \"speech_transformer.transformer.attn_layers.layers.8.1.wrap.to_k.weight\", \"speech_transformer.transformer.attn_layers.layers.8.1.wrap.to_v.weight\", \"speech_transformer.transformer.attn_layers.layers.8.1.wrap.to_out.weight\", \"speech_transformer.transformer.attn_layers.layers.8.1.wrap.to_out.bias\", \"speech_transformer.transformer.attn_layers.layers.9.0.0.g\", \"speech_transformer.transformer.attn_layers.layers.9.1.wrap.net.0.proj.weight\", \"speech_transformer.transformer.attn_layers.layers.9.1.wrap.net.0.proj.bias\", \"speech_transformer.transformer.attn_layers.layers.9.1.wrap.net.3.weight\", \"speech_transformer.transformer.attn_layers.layers.9.1.wrap.net.3.bias\", \"speech_transformer.transformer.attn_layers.layers.10.0.0.g\", \"speech_transformer.transformer.attn_layers.layers.10.1.wrap.to_q.weight\", \"speech_transformer.transformer.attn_layers.layers.10.1.wrap.to_k.weight\", \"speech_transformer.transformer.attn_layers.layers.10.1.wrap.to_v.weight\", \"speech_transformer.transformer.attn_layers.layers.10.1.wrap.to_out.weight\", \"speech_transformer.transformer.attn_layers.layers.10.1.wrap.to_out.bias\", \"speech_transformer.transformer.attn_layers.layers.11.0.0.g\", \"speech_transformer.transformer.attn_layers.layers.11.1.wrap.net.0.proj.weight\", \"speech_transformer.transformer.attn_layers.layers.11.1.wrap.net.0.proj.bias\", \"speech_transformer.transformer.attn_layers.layers.11.1.wrap.net.3.weight\", \"speech_transformer.transformer.attn_layers.layers.11.1.wrap.net.3.bias\", \"speech_transformer.transformer.attn_layers.layers.12.0.0.g\", \"speech_transformer.transformer.attn_layers.layers.12.1.wrap.to_q.weight\", \"speech_transformer.transformer.attn_layers.layers.12.1.wrap.to_k.weight\", \"speech_transformer.transformer.attn_layers.layers.12.1.wrap.to_v.weight\", \"speech_transformer.transformer.attn_layers.layers.12.1.wrap.to_out.weight\", \"speech_transformer.transformer.attn_layers.layers.12.1.wrap.to_out.bias\", \"speech_transformer.transformer.attn_layers.layers.13.0.0.g\", \"speech_transformer.transformer.attn_layers.layers.13.1.wrap.net.0.proj.weight\", \"speech_transformer.transformer.attn_layers.layers.13.1.wrap.net.0.proj.bias\", \"speech_transformer.transformer.attn_layers.layers.13.1.wrap.net.3.weight\", \"speech_transformer.transformer.attn_layers.layers.13.1.wrap.net.3.bias\", \"speech_transformer.transformer.attn_layers.layers.14.0.0.g\", \"speech_transformer.transformer.attn_layers.layers.14.1.wrap.to_q.weight\", \"speech_transformer.transformer.attn_layers.layers.14.1.wrap.to_k.weight\", \"speech_transformer.transformer.attn_layers.layers.14.1.wrap.to_v.weight\", \"speech_transformer.transformer.attn_layers.layers.14.1.wrap.to_out.weight\", \"speech_transformer.transformer.attn_layers.layers.14.1.wrap.to_out.bias\", \"speech_transformer.transformer.attn_layers.layers.15.0.0.g\", \"speech_transformer.transformer.attn_layers.layers.15.1.wrap.net.0.proj.weight\", \"speech_transformer.transformer.attn_layers.layers.15.1.wrap.net.0.proj.bias\", \"speech_transformer.transformer.attn_layers.layers.15.1.wrap.net.3.weight\", \"speech_transformer.transformer.attn_layers.layers.15.1.wrap.net.3.bias\", \"speech_transformer.transformer.attn_layers.layers.16.0.0.g\", \"speech_transformer.transformer.attn_layers.layers.16.1.wrap.to_q.weight\", \"speech_transformer.transformer.attn_layers.layers.16.1.wrap.to_k.weight\", \"speech_transformer.transformer.attn_layers.layers.16.1.wrap.to_v.weight\", \"speech_transformer.transformer.attn_layers.layers.16.1.wrap.to_out.weight\", \"speech_transformer.transformer.attn_layers.layers.16.1.wrap.to_out.bias\", \"speech_transformer.transformer.attn_layers.layers.17.0.0.g\", \"speech_transformer.transformer.attn_layers.layers.17.1.wrap.net.0.proj.weight\", \"speech_transformer.transformer.attn_layers.layers.17.1.wrap.net.0.proj.bias\", \"speech_transformer.transformer.attn_layers.layers.17.1.wrap.net.3.weight\", \"speech_transformer.transformer.attn_layers.layers.17.1.wrap.net.3.bias\", \"speech_transformer.transformer.attn_layers.layers.18.0.0.g\", \"speech_transformer.transformer.attn_layers.layers.18.1.wrap.to_q.weight\", \"speech_transformer.transformer.attn_layers.layers.18.1.wrap.to_k.weight\", \"speech_transformer.transformer.attn_layers.layers.18.1.wrap.to_v.weight\", \"speech_transformer.transformer.attn_layers.layers.18.1.wrap.to_out.weight\", \"speech_transformer.transformer.attn_layers.layers.18.1.wrap.to_out.bias\", \"speech_transformer.transformer.attn_layers.layers.19.0.0.g\", \"speech_transformer.transformer.attn_layers.layers.19.1.wrap.net.0.proj.weight\", \"speech_transformer.transformer.attn_layers.layers.19.1.wrap.net.0.proj.bias\", \"speech_transformer.transformer.attn_layers.layers.19.1.wrap.net.3.weight\", \"speech_transformer.transformer.attn_layers.layers.19.1.wrap.net.3.bias\", \"speech_transformer.transformer.attn_layers.layers.20.0.0.g\", \"speech_transformer.transformer.attn_layers.layers.20.1.wrap.to_q.weight\", \"speech_transformer.transformer.attn_layers.layers.20.1.wrap.to_k.weight\", \"speech_transformer.transformer.attn_layers.layers.20.1.wrap.to_v.weight\", \"speech_transformer.transformer.attn_layers.layers.20.1.wrap.to_out.weight\", \"speech_transformer.transformer.attn_layers.layers.20.1.wrap.to_out.bias\", \"speech_transformer.transformer.attn_layers.layers.21.0.0.g\", \"speech_transformer.transformer.attn_layers.layers.21.1.wrap.net.0.proj.weight\", \"speech_transformer.transformer.attn_layers.layers.21.1.wrap.net.0.proj.bias\", \"speech_transformer.transformer.attn_layers.layers.21.1.wrap.net.3.weight\", \"speech_transformer.transformer.attn_layers.layers.21.1.wrap.net.3.bias\", \"speech_transformer.transformer.attn_layers.layers.22.0.0.g\", \"speech_transformer.transformer.attn_layers.layers.22.1.wrap.to_q.weight\", \"speech_transformer.transformer.attn_layers.layers.22.1.wrap.to_k.weight\", \"speech_transformer.transformer.attn_layers.layers.22.1.wrap.to_v.weight\", \"speech_transformer.transformer.attn_layers.layers.22.1.wrap.to_out.weight\", \"speech_transformer.transformer.attn_layers.layers.22.1.wrap.to_out.bias\", \"speech_transformer.transformer.attn_layers.layers.23.0.0.g\", \"speech_transformer.transformer.attn_layers.layers.23.1.wrap.net.0.proj.weight\", \"speech_transformer.transformer.attn_layers.layers.23.1.wrap.net.0.proj.bias\", \"speech_transformer.transformer.attn_layers.layers.23.1.wrap.net.3.weight\", \"speech_transformer.transformer.attn_layers.layers.23.1.wrap.net.3.bias\", \"speech_transformer.transformer.attn_layers.rotary_pos_emb.inv_freq\", \"speech_transformer.transformer.norm.weight\", \"speech_transformer.transformer.norm.bias\". \n\tsize mismatch for text_emb.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([148, 512]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m pretrained_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m./data/clvp.pth\u001b[39m\u001b[39m'\u001b[39m  \u001b[39m# Replace with the actual path to clvp.pth\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m# Initialize the CLVP metric with the correct device (e.g., 'cuda') and the path to the model\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m cv_metric \u001b[39m=\u001b[39m CLVPMetric(device\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m'\u001b[39;49m, pretrained_path\u001b[39m=\u001b[39;49mpretrained_path)\n\u001b[1;32m      9\u001b[0m \u001b[39m# Now you can use cv_metric as intended\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tts_scores/clvp.py:359\u001b[0m, in \u001b[0;36mCLVPMetric.__init__\u001b[0;34m(self, device, pretrained_path)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m CLVP(dim_text\u001b[39m=\u001b[39m\u001b[39m512\u001b[39m, dim_latent\u001b[39m=\u001b[39m\u001b[39m512\u001b[39m, dim_speech\u001b[39m=\u001b[39m\u001b[39m512\u001b[39m, num_text_tokens\u001b[39m=\u001b[39m\u001b[39m148\u001b[39m, text_enc_depth\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m,\n\u001b[1;32m    357\u001b[0m                   text_seq_len\u001b[39m=\u001b[39m\u001b[39m400\u001b[39m, text_heads\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m, speech_enc_depth\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, speech_heads\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m, speech_seq_len\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m)\u001b[39m.\u001b[39meval()\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m    358\u001b[0m sd \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(pretrained_path, map_location\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m--> 359\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mload_state_dict(sd)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   2036\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[1;32m   2037\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   2038\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2040\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 2041\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   2042\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2043\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for CLVP:\n\tMissing key(s) in state_dict: \"text_pos_emb.weight\", \"text_transformer.layers.layers.0.0.scale\", \"text_transformer.layers.layers.0.0.fn.norm.weight\", \"text_transformer.layers.layers.0.0.fn.norm.bias\", \"text_transformer.layers.layers.0.0.fn.fn.to_qkv.weight\", \"text_transformer.layers.layers.0.0.fn.fn.to_out.0.weight\", \"text_transformer.layers.layers.0.0.fn.fn.to_out.0.bias\", \"text_transformer.layers.layers.0.1.scale\", \"text_transformer.layers.layers.0.1.fn.norm.weight\", \"text_transformer.layers.layers.0.1.fn.norm.bias\", \"text_transformer.layers.layers.0.1.fn.fn.net.0.weight\", \"text_transformer.layers.layers.0.1.fn.fn.net.0.bias\", \"text_transformer.layers.layers.0.1.fn.fn.net.3.weight\", \"text_transformer.layers.layers.0.1.fn.fn.net.3.bias\", \"text_transformer.layers.layers.1.0.scale\", \"text_transformer.layers.layers.1.0.fn.norm.weight\", \"text_transformer.layers.layers.1.0.fn.norm.bias\", \"text_transformer.layers.layers.1.0.fn.fn.to_qkv.weight\", \"text_transformer.layers.layers.1.0.fn.fn.to_out.0.weight\", \"text_transformer.layers.layers.1.0.fn.fn.to_out.0.bias\", \"text_transformer.layers.layers.1.1.scale\", \"text_transformer.layers.layers.1.1.fn.norm.weight\", \"text_transformer.layers.layers.1.1.fn.norm.bias\", \"text_transformer.layers.layers.1.1.fn.fn.net.0.weight\", \"text_transformer.layers.layers.1.1.fn.fn.net.0.bias\", \"text_transformer.layers.layers.1.1.fn.fn.net.3.weight\", \"text_transformer.layers.layers.1.1.fn.fn.net.3.bias\", \"text_transformer.layers.layers.2.0.scale\", \"text_transformer.layers.layers.2.0.fn.norm.weight\", \"text_transformer.layers.layers.2.0.fn.norm.bias\", \"text_transformer.layers.layers.2.0.fn.fn.to_qkv.weight\", \"text_transformer.layers.layers.2.0.fn.fn.to_out.0.weight\", \"text_transformer.layers.layers.2.0.fn.fn.to_out.0.bias\", \"text_transformer.layers.layers.2.1.scale\", \"text_transformer.layers.layers.2.1.fn.norm.weight\", \"text_transformer.layers.layers.2.1.fn.norm.bias\", \"text_transformer.layers.layers.2.1.fn.fn.net.0.weight\", \"text_transformer.layers.layers.2.1.fn.fn.net.0.bias\", \"text_transformer.layers.layers.2.1.fn.fn.net.3.weight\", \"text_transformer.layers.layers.2.1.fn.fn.net.3.bias\", \"text_transformer.layers.layers.3.0.scale\", \"text_transformer.layers.layers.3.0.fn.norm.weight\", \"text_transformer.layers.layers.3.0.fn.norm.bias\", \"text_transformer.layers.layers.3.0.fn.fn.to_qkv.weight\", \"text_transformer.layers.layers.3.0.fn.fn.to_out.0.weight\", \"text_transformer.layers.layers.3.0.fn.fn.to_out.0.bias\", \"text_transformer.layers.layers.3.1.scale\", \"text_transformer.layers.layers.3.1.fn.norm.weight\", \"text_transformer.layers.layers.3.1.fn.norm.bias\", \"text_transformer.layers.layers.3.1.fn.fn.net.0.weight\", \"text_transformer.layers.layers.3.1.fn.fn.net.0.bias\", \"text_transformer.layers.layers.3.1.fn.fn.net.3.weight\", \"text_transformer.layers.layers.3.1.fn.fn.net.3.bias\", \"text_transformer.layers.layers.4.0.scale\", \"text_transformer.layers.layers.4.0.fn.norm.weight\", \"text_transformer.layers.layers.4.0.fn.norm.bias\", \"text_transformer.layers.layers.4.0.fn.fn.to_qkv.weight\", \"text_transformer.layers.layers.4.0.fn.fn.to_out.0.weight\", \"text_transformer.layers.layers.4.0.fn.fn.to_out.0.bias\", \"text_transformer.layers.layers.4.1.scale\", \"text_transformer.layers.layers.4.1.fn.norm.weight\", \"text_transformer.layers.layers.4.1.fn.norm.bias\", \"text_transformer.layers.layers.4.1.fn.fn.net.0.weight\", \"text_transformer.layers.layers.4.1.fn.fn.net.0.bias\", \"text_transformer.layers.layers.4.1.fn.fn.net.3.weight\", \"text_transformer.layers.layers.4.1.fn.fn.net.3.bias\", \"text_transformer.layers.layers.5.0.scale\", \"text_transformer.layers.layers.5.0.fn.norm.weight\", \"text_transformer.layers.layers.5.0.fn.norm.bias\", \"text_transformer.layers.layers.5.0.fn.fn.to_qkv.weight\", \"text_transformer.layers.layers.5.0.fn.fn.to_out.0.weight\", \"text_transformer.layers.layers.5.0.fn.fn.to_out.0.bias\", \"text_transformer.layers.layers.5.1.scale\", \"text_transformer.layers.layers.5.1.fn.norm.weight\", \"text_transformer.layers.layers.5.1.fn.norm.bias\", \"text_transformer.layers.layers.5.1.fn.fn.net.0.weight\", \"text_transformer.layers.layers.5.1.fn.fn.net.0.bias\", \"text_transformer.layers.layers.5.1.fn.fn.net.3.weight\", \"text_transformer.layers.layers.5.1.fn.fn.net.3.bias\", \"text_transformer.layers.layers.6.0.scale\", \"text_transformer.layers.layers.6.0.fn.norm.weight\", \"text_transformer.layers.layers.6.0.fn.norm.bias\", \"text_transformer.layers.layers.6.0.fn.fn.to_qkv.weight\", \"text_transformer.layers.layers.6.0.fn.fn.to_out.0.weight\", \"text_transformer.layers.layers.6.0.fn.fn.to_out.0.bias\", \"text_transformer.layers.layers.6.1.scale\", \"text_transformer.layers.layers.6.1.fn.norm.weight\", \"text_transformer.layers.layers.6.1.fn.norm.bias\", \"text_transformer.layers.layers.6.1.fn.fn.net.0.weight\", \"text_transformer.layers.layers.6.1.fn.fn.net.0.bias\", \"text_transformer.layers.layers.6.1.fn.fn.net.3.weight\", \"text_transformer.layers.layers.6.1.fn.fn.net.3.bias\", \"text_transformer.layers.layers.7.0.scale\", \"text_transformer.layers.layers.7.0.fn.norm.weight\", \"text_transformer.layers.layers.7.0.fn.norm.bias\", \"text_transformer.layers.layers.7.0.fn.fn.to_qkv.weight\", \"text_transformer.layers.layers.7.0.fn.fn.to_out.0.weight\", \"text_transformer.layers.layers.7.0.fn.fn.to_out.0.bias\", \"text_transformer.layers.layers.7.1.scale\", \"text_transformer.layers.layers.7.1.fn.norm.weight\", \"text_transformer.layers.layers.7.1.fn.norm.bias\", \"text_transformer.layers.layers.7.1.fn.fn.net.0.weight\", \"text_transformer.layers.layers.7.1.fn.fn.net.0.bias\", \"text_transformer.layers.layers.7.1.fn.fn.net.3.weight\", \"text_transformer.layers.layers.7.1.fn.fn.net.3.bias\", \"speech_enc.weight\", \"speech_enc.bias\", \"speech_pos_emb.weight\", \"speech_transformer.layers.layers.0.0.scale\", \"speech_transformer.layers.layers.0.0.fn.norm.weight\", \"speech_transformer.layers.layers.0.0.fn.norm.bias\", \"speech_transformer.layers.layers.0.0.fn.fn.to_qkv.weight\", \"speech_transformer.layers.layers.0.0.fn.fn.to_out.0.weight\", \"speech_transformer.layers.layers.0.0.fn.fn.to_out.0.bias\", \"speech_transformer.layers.layers.0.1.scale\", \"speech_transformer.layers.layers.0.1.fn.norm.weight\", \"speech_transformer.layers.layers.0.1.fn.norm.bias\", \"speech_transformer.layers.layers.0.1.fn.fn.net.0.weight\", \"speech_transformer.layers.layers.0.1.fn.fn.net.0.bias\", \"speech_transformer.layers.layers.0.1.fn.fn.net.3.weight\", \"speech_transformer.layers.layers.0.1.fn.fn.net.3.bias\", \"speech_transformer.layers.layers.1.0.scale\", \"speech_transformer.layers.layers.1.0.fn.norm.weight\", \"speech_transformer.layers.layers.1.0.fn.norm.bias\", \"speech_transformer.layers.layers.1.0.fn.fn.to_qkv.weight\", \"speech_transformer.layers.layers.1.0.fn.fn.to_out.0.weight\", \"speech_transformer.layers.layers.1.0.fn.fn.to_out.0.bias\", \"speech_transformer.layers.layers.1.1.scale\", \"speech_transformer.layers.layers.1.1.fn.norm.weight\", \"speech_transformer.layers.layers.1.1.fn.norm.bias\", \"speech_transformer.layers.layers.1.1.fn.fn.net.0.weight\", \"speech_transformer.layers.layers.1.1.fn.fn.net.0.bias\", \"speech_transformer.layers.layers.1.1.fn.fn.net.3.weight\", \"speech_transformer.layers.layers.1.1.fn.fn.net.3.bias\", \"speech_transformer.layers.layers.2.0.scale\", \"speech_transformer.layers.layers.2.0.fn.norm.weight\", \"speech_transformer.layers.layers.2.0.fn.norm.bias\", \"speech_transformer.layers.layers.2.0.fn.fn.to_qkv.weight\", \"speech_transformer.layers.layers.2.0.fn.fn.to_out.0.weight\", \"speech_transformer.layers.layers.2.0.fn.fn.to_out.0.bias\", \"speech_transformer.layers.layers.2.1.scale\", \"speech_transformer.layers.layers.2.1.fn.norm.weight\", \"speech_transformer.layers.layers.2.1.fn.norm.bias\", \"speech_transformer.layers.layers.2.1.fn.fn.net.0.weight\", \"speech_transformer.layers.layers.2.1.fn.fn.net.0.bias\", \"speech_transformer.layers.layers.2.1.fn.fn.net.3.weight\", \"speech_transformer.layers.layers.2.1.fn.fn.net.3.bias\", \"speech_transformer.layers.layers.3.0.scale\", \"speech_transformer.layers.layers.3.0.fn.norm.weight\", \"speech_transformer.layers.layers.3.0.fn.norm.bias\", \"speech_transformer.layers.layers.3.0.fn.fn.to_qkv.weight\", \"speech_transformer.layers.layers.3.0.fn.fn.to_out.0.weight\", \"speech_transformer.layers.layers.3.0.fn.fn.to_out.0.bias\", \"speech_transformer.layers.layers.3.1.scale\", \"speech_transformer.layers.layers.3.1.fn.norm.weight\", \"speech_transformer.layers.layers.3.1.fn.norm.bias\", \"speech_transformer.layers.layers.3.1.fn.fn.net.0.weight\", \"speech_transformer.layers.layers.3.1.fn.fn.net.0.bias\", \"speech_transformer.layers.layers.3.1.fn.fn.net.3.weight\", \"speech_transformer.layers.layers.3.1.fn.fn.net.3.bias\", \"speech_transformer.layers.layers.4.0.scale\", \"speech_transformer.layers.layers.4.0.fn.norm.weight\", \"speech_transformer.layers.layers.4.0.fn.norm.bias\", \"speech_transformer.layers.layers.4.0.fn.fn.to_qkv.weight\", \"speech_transformer.layers.layers.4.0.fn.fn.to_out.0.weight\", \"speech_transformer.layers.layers.4.0.fn.fn.to_out.0.bias\", \"speech_transformer.layers.layers.4.1.scale\", \"speech_transformer.layers.layers.4.1.fn.norm.weight\", \"speech_transformer.layers.layers.4.1.fn.norm.bias\", \"speech_transformer.layers.layers.4.1.fn.fn.net.0.weight\", \"speech_transformer.layers.layers.4.1.fn.fn.net.0.bias\", \"speech_transformer.layers.layers.4.1.fn.fn.net.3.weight\", \"speech_transformer.layers.layers.4.1.fn.fn.net.3.bias\", \"speech_transformer.layers.layers.5.0.scale\", \"speech_transformer.layers.layers.5.0.fn.norm.weight\", \"speech_transformer.layers.layers.5.0.fn.norm.bias\", \"speech_transformer.layers.layers.5.0.fn.fn.to_qkv.weight\", \"speech_transformer.layers.layers.5.0.fn.fn.to_out.0.weight\", \"speech_transformer.layers.layers.5.0.fn.fn.to_out.0.bias\", \"speech_transformer.layers.layers.5.1.scale\", \"speech_transformer.layers.layers.5.1.fn.norm.weight\", \"speech_transformer.layers.layers.5.1.fn.norm.bias\", \"speech_transformer.layers.layers.5.1.fn.fn.net.0.weight\", \"speech_transformer.layers.layers.5.1.fn.fn.net.0.bias\", \"speech_transformer.layers.layers.5.1.fn.fn.net.3.weight\", \"speech_transformer.layers.layers.5.1.fn.fn.net.3.bias\", \"speech_transformer.layers.layers.6.0.scale\", \"speech_transformer.layers.layers.6.0.fn.norm.weight\", \"speech_transformer.layers.layers.6.0.fn.norm.bias\", \"speech_transformer.layers.layers.6.0.fn.fn.to_qkv.weight\", \"speech_transformer.layers.layers.6.0.fn.fn.to_out.0.weight\", \"speech_transformer.layers.layers.6.0.fn.fn.to_out.0.bias\", \"speech_transformer.layers.layers.6.1.scale\", \"speech_transformer.layers.layers.6.1.fn.norm.weight\", \"speech_transformer.layers.layers.6.1.fn.norm.bias\", \"speech_transformer.layers.layers.6.1.fn.fn.net.0.weight\", \"speech_transformer.layers.layers.6.1.fn.fn.net.0.bias\", \"speech_transformer.layers.layers.6.1.fn.fn.net.3.weight\", \"speech_transformer.layers.layers.6.1.fn.fn.net.3.bias\", \"speech_transformer.layers.layers.7.0.scale\", \"speech_transformer.layers.layers.7.0.fn.norm.weight\", \"speech_transformer.layers.layers.7.0.fn.norm.bias\", \"speech_transformer.layers.layers.7.0.fn.fn.to_qkv.weight\", \"speech_transformer.layers.layers.7.0.fn.fn.to_out.0.weight\", \"speech_transformer.layers.layers.7.0.fn.fn.to_out.0.bias\", \"speech_transformer.layers.layers.7.1.scale\", \"speech_transformer.layers.layers.7.1.fn.norm.weight\", \"speech_transformer.layers.layers.7.1.fn.norm.bias\", \"speech_transformer.layers.layers.7.1.fn.fn.net.0.weight\", \"speech_transformer.layers.layers.7.1.fn.fn.net.0.bias\", \"speech_transformer.layers.layers.7.1.fn.fn.net.3.weight\", \"speech_transformer.layers.layers.7.1.fn.fn.net.3.bias\", \"speech_transformer.layers.layers.8.0.scale\", \"speech_transformer.layers.layers.8.0.fn.norm.weight\", \"speech_transformer.layers.layers.8.0.fn.norm.bias\", \"speech_transformer.layers.layers.8.0.fn.fn.to_qkv.weight\", \"speech_transformer.layers.layers.8.0.fn.fn.to_out.0.weight\", \"speech_transformer.layers.layers.8.0.fn.fn.to_out.0.bias\", \"speech_transformer.layers.layers.8.1.scale\", \"speech_transformer.layers.layers.8.1.fn.norm.weight\", \"speech_transformer.layers.layers.8.1.fn.norm.bias\", \"speech_transformer.layers.layers.8.1.fn.fn.net.0.weight\", \"speech_transformer.layers.layers.8.1.fn.fn.net.0.bias\", \"speech_transformer.layers.layers.8.1.fn.fn.net.3.weight\", \"speech_transformer.layers.layers.8.1.fn.fn.net.3.bias\", \"speech_transformer.layers.layers.9.0.scale\", \"speech_transformer.layers.layers.9.0.fn.norm.weight\", \"speech_transformer.layers.layers.9.0.fn.norm.bias\", \"speech_transformer.layers.layers.9.0.fn.fn.to_qkv.weight\", \"speech_transformer.layers.layers.9.0.fn.fn.to_out.0.weight\", \"speech_transformer.layers.layers.9.0.fn.fn.to_out.0.bias\", \"speech_transformer.layers.layers.9.1.scale\", \"speech_transformer.layers.layers.9.1.fn.norm.weight\", \"speech_transformer.layers.layers.9.1.fn.norm.bias\", \"speech_transformer.layers.layers.9.1.fn.fn.net.0.weight\", \"speech_transformer.layers.layers.9.1.fn.fn.net.0.bias\", \"speech_transformer.layers.layers.9.1.fn.fn.net.3.weight\", \"speech_transformer.layers.layers.9.1.fn.fn.net.3.bias\". \n\tUnexpected key(s) in state_dict: \"speech_emb.weight\", \"text_transformer.transformer.attn_layers.layers.0.0.0.g\", \"text_transformer.transformer.attn_layers.layers.0.1.wrap.to_q.weight\", \"text_transformer.transformer.attn_layers.layers.0.1.wrap.to_k.weight\", \"text_transformer.transformer.attn_layers.layers.0.1.wrap.to_v.weight\", \"text_transformer.transformer.attn_layers.layers.0.1.wrap.to_out.weight\", \"text_transformer.transformer.attn_layers.layers.0.1.wrap.to_out.bias\", \"text_transformer.transformer.attn_layers.layers.1.0.0.g\", \"text_transformer.transformer.attn_layers.layers.1.1.wrap.net.0.proj.weight\", \"text_transformer.transformer.attn_layers.layers.1.1.wrap.net.0.proj.bias\", \"text_transformer.transformer.attn_layers.layers.1.1.wrap.net.3.weight\", \"text_transformer.transformer.attn_layers.layers.1.1.wrap.net.3.bias\", \"text_transformer.transformer.attn_layers.layers.2.0.0.g\", \"text_transformer.transformer.attn_layers.layers.2.1.wrap.to_q.weight\", \"text_transformer.transformer.attn_layers.layers.2.1.wrap.to_k.weight\", \"text_transformer.transformer.attn_layers.layers.2.1.wrap.to_v.weight\", \"text_transformer.transformer.attn_layers.layers.2.1.wrap.to_out.weight\", \"text_transformer.transformer.attn_layers.layers.2.1.wrap.to_out.bias\", \"text_transformer.transformer.attn_layers.layers.3.0.0.g\", \"text_transformer.transformer.attn_layers.layers.3.1.wrap.net.0.proj.weight\", \"text_transformer.transformer.attn_layers.layers.3.1.wrap.net.0.proj.bias\", \"text_transformer.transformer.attn_layers.layers.3.1.wrap.net.3.weight\", \"text_transformer.transformer.attn_layers.layers.3.1.wrap.net.3.bias\", \"text_transformer.transformer.attn_layers.layers.4.0.0.g\", \"text_transformer.transformer.attn_layers.layers.4.1.wrap.to_q.weight\", \"text_transformer.transformer.attn_layers.layers.4.1.wrap.to_k.weight\", \"text_transformer.transformer.attn_layers.layers.4.1.wrap.to_v.weight\", \"text_transformer.transformer.attn_layers.layers.4.1.wrap.to_out.weight\", \"text_transformer.transformer.attn_layers.layers.4.1.wrap.to_out.bias\", \"text_transformer.transformer.attn_layers.layers.5.0.0.g\", \"text_transformer.transformer.attn_layers.layers.5.1.wrap.net.0.proj.weight\", \"text_transformer.transformer.attn_layers.layers.5.1.wrap.net.0.proj.bias\", \"text_transformer.transformer.attn_layers.layers.5.1.wrap.net.3.weight\", \"text_transformer.transformer.attn_layers.layers.5.1.wrap.net.3.bias\", \"text_transformer.transformer.attn_layers.layers.6.0.0.g\", \"text_transformer.transformer.attn_layers.layers.6.1.wrap.to_q.weight\", \"text_transformer.transformer.attn_layers.layers.6.1.wrap.to_k.weight\", \"text_transformer.transformer.attn_layers.layers.6.1.wrap.to_v.weight\", \"text_transformer.transformer.attn_layers.layers.6.1.wrap.to_out.weight\", \"text_transformer.transformer.attn_layers.layers.6.1.wrap.to_out.bias\", \"text_transformer.transformer.attn_layers.layers.7.0.0.g\", \"text_transformer.transformer.attn_layers.layers.7.1.wrap.net.0.proj.weight\", \"text_transformer.transformer.attn_layers.layers.7.1.wrap.net.0.proj.bias\", \"text_transformer.transformer.attn_layers.layers.7.1.wrap.net.3.weight\", \"text_transformer.transformer.attn_layers.layers.7.1.wrap.net.3.bias\", \"text_transformer.transformer.attn_layers.layers.8.0.0.g\", \"text_transformer.transformer.attn_layers.layers.8.1.wrap.to_q.weight\", \"text_transformer.transformer.attn_layers.layers.8.1.wrap.to_k.weight\", \"text_transformer.transformer.attn_layers.layers.8.1.wrap.to_v.weight\", \"text_transformer.transformer.attn_layers.layers.8.1.wrap.to_out.weight\", \"text_transformer.transformer.attn_layers.layers.8.1.wrap.to_out.bias\", \"text_transformer.transformer.attn_layers.layers.9.0.0.g\", \"text_transformer.transformer.attn_layers.layers.9.1.wrap.net.0.proj.weight\", \"text_transformer.transformer.attn_layers.layers.9.1.wrap.net.0.proj.bias\", \"text_transformer.transformer.attn_layers.layers.9.1.wrap.net.3.weight\", \"text_transformer.transformer.attn_layers.layers.9.1.wrap.net.3.bias\", \"text_transformer.transformer.attn_layers.layers.10.0.0.g\", \"text_transformer.transformer.attn_layers.layers.10.1.wrap.to_q.weight\", \"text_transformer.transformer.attn_layers.layers.10.1.wrap.to_k.weight\", \"text_transformer.transformer.attn_layers.layers.10.1.wrap.to_v.weight\", \"text_transformer.transformer.attn_layers.layers.10.1.wrap.to_out.weight\", \"text_transformer.transformer.attn_layers.layers.10.1.wrap.to_out.bias\", \"text_transformer.transformer.attn_layers.layers.11.0.0.g\", \"text_transformer.transformer.attn_layers.layers.11.1.wrap.net.0.proj.weight\", \"text_transformer.transformer.attn_layers.layers.11.1.wrap.net.0.proj.bias\", \"text_transformer.transformer.attn_layers.layers.11.1.wrap.net.3.weight\", \"text_transformer.transformer.attn_layers.layers.11.1.wrap.net.3.bias\", \"text_transformer.transformer.attn_layers.layers.12.0.0.g\", \"text_transformer.transformer.attn_layers.layers.12.1.wrap.to_q.weight\", \"text_transformer.transformer.attn_layers.layers.12.1.wrap.to_k.weight\", \"text_transformer.transformer.attn_layers.layers.12.1.wrap.to_v.weight\", \"text_transformer.transformer.attn_layers.layers.12.1.wrap.to_out.weight\", \"text_transformer.transformer.attn_layers.layers.12.1.wrap.to_out.bias\", \"text_transformer.transformer.attn_layers.layers.13.0.0.g\", \"text_transformer.transformer.attn_layers.layers.13.1.wrap.net.0.proj.weight\", \"text_transformer.transformer.attn_layers.layers.13.1.wrap.net.0.proj.bias\", \"text_transformer.transformer.attn_layers.layers.13.1.wrap.net.3.weight\", \"text_transformer.transformer.attn_layers.layers.13.1.wrap.net.3.bias\", \"text_transformer.transformer.attn_layers.layers.14.0.0.g\", \"text_transformer.transformer.attn_layers.layers.14.1.wrap.to_q.weight\", \"text_transformer.transformer.attn_layers.layers.14.1.wrap.to_k.weight\", \"text_transformer.transformer.attn_layers.layers.14.1.wrap.to_v.weight\", \"text_transformer.transformer.attn_layers.layers.14.1.wrap.to_out.weight\", \"text_transformer.transformer.attn_layers.layers.14.1.wrap.to_out.bias\", \"text_transformer.transformer.attn_layers.layers.15.0.0.g\", \"text_transformer.transformer.attn_layers.layers.15.1.wrap.net.0.proj.weight\", \"text_transformer.transformer.attn_layers.layers.15.1.wrap.net.0.proj.bias\", \"text_transformer.transformer.attn_layers.layers.15.1.wrap.net.3.weight\", \"text_transformer.transformer.attn_layers.layers.15.1.wrap.net.3.bias\", \"text_transformer.transformer.attn_layers.layers.16.0.0.g\", \"text_transformer.transformer.attn_layers.layers.16.1.wrap.to_q.weight\", \"text_transformer.transformer.attn_layers.layers.16.1.wrap.to_k.weight\", \"text_transformer.transformer.attn_layers.layers.16.1.wrap.to_v.weight\", \"text_transformer.transformer.attn_layers.layers.16.1.wrap.to_out.weight\", \"text_transformer.transformer.attn_layers.layers.16.1.wrap.to_out.bias\", \"text_transformer.transformer.attn_layers.layers.17.0.0.g\", \"text_transformer.transformer.attn_layers.layers.17.1.wrap.net.0.proj.weight\", \"text_transformer.transformer.attn_layers.layers.17.1.wrap.net.0.proj.bias\", \"text_transformer.transformer.attn_layers.layers.17.1.wrap.net.3.weight\", \"text_transformer.transformer.attn_layers.layers.17.1.wrap.net.3.bias\", \"text_transformer.transformer.attn_layers.layers.18.0.0.g\", \"text_transformer.transformer.attn_layers.layers.18.1.wrap.to_q.weight\", \"text_transformer.transformer.attn_layers.layers.18.1.wrap.to_k.weight\", \"text_transformer.transformer.attn_layers.layers.18.1.wrap.to_v.weight\", \"text_transformer.transformer.attn_layers.layers.18.1.wrap.to_out.weight\", \"text_transformer.transformer.attn_layers.layers.18.1.wrap.to_out.bias\", \"text_transformer.transformer.attn_layers.layers.19.0.0.g\", \"text_transformer.transformer.attn_layers.layers.19.1.wrap.net.0.proj.weight\", \"text_transformer.transformer.attn_layers.layers.19.1.wrap.net.0.proj.bias\", \"text_transformer.transformer.attn_layers.layers.19.1.wrap.net.3.weight\", \"text_transformer.transformer.attn_layers.layers.19.1.wrap.net.3.bias\", \"text_transformer.transformer.attn_layers.layers.20.0.0.g\", \"text_transformer.transformer.attn_layers.layers.20.1.wrap.to_q.weight\", \"text_transformer.transformer.attn_layers.layers.20.1.wrap.to_k.weight\", \"text_transformer.transformer.attn_layers.layers.20.1.wrap.to_v.weight\", \"text_transformer.transformer.attn_layers.layers.20.1.wrap.to_out.weight\", \"text_transformer.transformer.attn_layers.layers.20.1.wrap.to_out.bias\", \"text_transformer.transformer.attn_layers.layers.21.0.0.g\", \"text_transformer.transformer.attn_layers.layers.21.1.wrap.net.0.proj.weight\", \"text_transformer.transformer.attn_layers.layers.21.1.wrap.net.0.proj.bias\", \"text_transformer.transformer.attn_layers.layers.21.1.wrap.net.3.weight\", \"text_transformer.transformer.attn_layers.layers.21.1.wrap.net.3.bias\", \"text_transformer.transformer.attn_layers.layers.22.0.0.g\", \"text_transformer.transformer.attn_layers.layers.22.1.wrap.to_q.weight\", \"text_transformer.transformer.attn_layers.layers.22.1.wrap.to_k.weight\", \"text_transformer.transformer.attn_layers.layers.22.1.wrap.to_v.weight\", \"text_transformer.transformer.attn_layers.layers.22.1.wrap.to_out.weight\", \"text_transformer.transformer.attn_layers.layers.22.1.wrap.to_out.bias\", \"text_transformer.transformer.attn_layers.layers.23.0.0.g\", \"text_transformer.transformer.attn_layers.layers.23.1.wrap.net.0.proj.weight\", \"text_transformer.transformer.attn_layers.layers.23.1.wrap.net.0.proj.bias\", \"text_transformer.transformer.attn_layers.layers.23.1.wrap.net.3.weight\", \"text_transformer.transformer.attn_layers.layers.23.1.wrap.net.3.bias\", \"text_transformer.transformer.attn_layers.rotary_pos_emb.inv_freq\", \"text_transformer.transformer.norm.weight\", \"text_transformer.transformer.norm.bias\", \"speech_transformer.transformer.attn_layers.layers.0.0.0.g\", \"speech_transformer.transformer.attn_layers.layers.0.1.wrap.to_q.weight\", \"speech_transformer.transformer.attn_layers.layers.0.1.wrap.to_k.weight\", \"speech_transformer.transformer.attn_layers.layers.0.1.wrap.to_v.weight\", \"speech_transformer.transformer.attn_layers.layers.0.1.wrap.to_out.weight\", \"speech_transformer.transformer.attn_layers.layers.0.1.wrap.to_out.bias\", \"speech_transformer.transformer.attn_layers.layers.1.0.0.g\", \"speech_transformer.transformer.attn_layers.layers.1.1.wrap.net.0.proj.weight\", \"speech_transformer.transformer.attn_layers.layers.1.1.wrap.net.0.proj.bias\", \"speech_transformer.transformer.attn_layers.layers.1.1.wrap.net.3.weight\", \"speech_transformer.transformer.attn_layers.layers.1.1.wrap.net.3.bias\", \"speech_transformer.transformer.attn_layers.layers.2.0.0.g\", \"speech_transformer.transformer.attn_layers.layers.2.1.wrap.to_q.weight\", \"speech_transformer.transformer.attn_layers.layers.2.1.wrap.to_k.weight\", \"speech_transformer.transformer.attn_layers.layers.2.1.wrap.to_v.weight\", \"speech_transformer.transformer.attn_layers.layers.2.1.wrap.to_out.weight\", \"speech_transformer.transformer.attn_layers.layers.2.1.wrap.to_out.bias\", \"speech_transformer.transformer.attn_layers.layers.3.0.0.g\", \"speech_transformer.transformer.attn_layers.layers.3.1.wrap.net.0.proj.weight\", \"speech_transformer.transformer.attn_layers.layers.3.1.wrap.net.0.proj.bias\", \"speech_transformer.transformer.attn_layers.layers.3.1.wrap.net.3.weight\", \"speech_transformer.transformer.attn_layers.layers.3.1.wrap.net.3.bias\", \"speech_transformer.transformer.attn_layers.layers.4.0.0.g\", \"speech_transformer.transformer.attn_layers.layers.4.1.wrap.to_q.weight\", \"speech_transformer.transformer.attn_layers.layers.4.1.wrap.to_k.weight\", \"speech_transformer.transformer.attn_layers.layers.4.1.wrap.to_v.weight\", \"speech_transformer.transformer.attn_layers.layers.4.1.wrap.to_out.weight\", \"speech_transformer.transformer.attn_layers.layers.4.1.wrap.to_out.bias\", \"speech_transformer.transformer.attn_layers.layers.5.0.0.g\", \"speech_transformer.transformer.attn_layers.layers.5.1.wrap.net.0.proj.weight\", \"speech_transformer.transformer.attn_layers.layers.5.1.wrap.net.0.proj.bias\", \"speech_transformer.transformer.attn_layers.layers.5.1.wrap.net.3.weight\", \"speech_transformer.transformer.attn_layers.layers.5.1.wrap.net.3.bias\", \"speech_transformer.transformer.attn_layers.layers.6.0.0.g\", \"speech_transformer.transformer.attn_layers.layers.6.1.wrap.to_q.weight\", \"speech_transformer.transformer.attn_layers.layers.6.1.wrap.to_k.weight\", \"speech_transformer.transformer.attn_layers.layers.6.1.wrap.to_v.weight\", \"speech_transformer.transformer.attn_layers.layers.6.1.wrap.to_out.weight\", \"speech_transformer.transformer.attn_layers.layers.6.1.wrap.to_out.bias\", \"speech_transformer.transformer.attn_layers.layers.7.0.0.g\", \"speech_transformer.transformer.attn_layers.layers.7.1.wrap.net.0.proj.weight\", \"speech_transformer.transformer.attn_layers.layers.7.1.wrap.net.0.proj.bias\", \"speech_transformer.transformer.attn_layers.layers.7.1.wrap.net.3.weight\", \"speech_transformer.transformer.attn_layers.layers.7.1.wrap.net.3.bias\", \"speech_transformer.transformer.attn_layers.layers.8.0.0.g\", \"speech_transformer.transformer.attn_layers.layers.8.1.wrap.to_q.weight\", \"speech_transformer.transformer.attn_layers.layers.8.1.wrap.to_k.weight\", \"speech_transformer.transformer.attn_layers.layers.8.1.wrap.to_v.weight\", \"speech_transformer.transformer.attn_layers.layers.8.1.wrap.to_out.weight\", \"speech_transformer.transformer.attn_layers.layers.8.1.wrap.to_out.bias\", \"speech_transformer.transformer.attn_layers.layers.9.0.0.g\", \"speech_transformer.transformer.attn_layers.layers.9.1.wrap.net.0.proj.weight\", \"speech_transformer.transformer.attn_layers.layers.9.1.wrap.net.0.proj.bias\", \"speech_transformer.transformer.attn_layers.layers.9.1.wrap.net.3.weight\", \"speech_transformer.transformer.attn_layers.layers.9.1.wrap.net.3.bias\", \"speech_transformer.transformer.attn_layers.layers.10.0.0.g\", \"speech_transformer.transformer.attn_layers.layers.10.1.wrap.to_q.weight\", \"speech_transformer.transformer.attn_layers.layers.10.1.wrap.to_k.weight\", \"speech_transformer.transformer.attn_layers.layers.10.1.wrap.to_v.weight\", \"speech_transformer.transformer.attn_layers.layers.10.1.wrap.to_out.weight\", \"speech_transformer.transformer.attn_layers.layers.10.1.wrap.to_out.bias\", \"speech_transformer.transformer.attn_layers.layers.11.0.0.g\", \"speech_transformer.transformer.attn_layers.layers.11.1.wrap.net.0.proj.weight\", \"speech_transformer.transformer.attn_layers.layers.11.1.wrap.net.0.proj.bias\", \"speech_transformer.transformer.attn_layers.layers.11.1.wrap.net.3.weight\", \"speech_transformer.transformer.attn_layers.layers.11.1.wrap.net.3.bias\", \"speech_transformer.transformer.attn_layers.layers.12.0.0.g\", \"speech_transformer.transformer.attn_layers.layers.12.1.wrap.to_q.weight\", \"speech_transformer.transformer.attn_layers.layers.12.1.wrap.to_k.weight\", \"speech_transformer.transformer.attn_layers.layers.12.1.wrap.to_v.weight\", \"speech_transformer.transformer.attn_layers.layers.12.1.wrap.to_out.weight\", \"speech_transformer.transformer.attn_layers.layers.12.1.wrap.to_out.bias\", \"speech_transformer.transformer.attn_layers.layers.13.0.0.g\", \"speech_transformer.transformer.attn_layers.layers.13.1.wrap.net.0.proj.weight\", \"speech_transformer.transformer.attn_layers.layers.13.1.wrap.net.0.proj.bias\", \"speech_transformer.transformer.attn_layers.layers.13.1.wrap.net.3.weight\", \"speech_transformer.transformer.attn_layers.layers.13.1.wrap.net.3.bias\", \"speech_transformer.transformer.attn_layers.layers.14.0.0.g\", \"speech_transformer.transformer.attn_layers.layers.14.1.wrap.to_q.weight\", \"speech_transformer.transformer.attn_layers.layers.14.1.wrap.to_k.weight\", \"speech_transformer.transformer.attn_layers.layers.14.1.wrap.to_v.weight\", \"speech_transformer.transformer.attn_layers.layers.14.1.wrap.to_out.weight\", \"speech_transformer.transformer.attn_layers.layers.14.1.wrap.to_out.bias\", \"speech_transformer.transformer.attn_layers.layers.15.0.0.g\", \"speech_transformer.transformer.attn_layers.layers.15.1.wrap.net.0.proj.weight\", \"speech_transformer.transformer.attn_layers.layers.15.1.wrap.net.0.proj.bias\", \"speech_transformer.transformer.attn_layers.layers.15.1.wrap.net.3.weight\", \"speech_transformer.transformer.attn_layers.layers.15.1.wrap.net.3.bias\", \"speech_transformer.transformer.attn_layers.layers.16.0.0.g\", \"speech_transformer.transformer.attn_layers.layers.16.1.wrap.to_q.weight\", \"speech_transformer.transformer.attn_layers.layers.16.1.wrap.to_k.weight\", \"speech_transformer.transformer.attn_layers.layers.16.1.wrap.to_v.weight\", \"speech_transformer.transformer.attn_layers.layers.16.1.wrap.to_out.weight\", \"speech_transformer.transformer.attn_layers.layers.16.1.wrap.to_out.bias\", \"speech_transformer.transformer.attn_layers.layers.17.0.0.g\", \"speech_transformer.transformer.attn_layers.layers.17.1.wrap.net.0.proj.weight\", \"speech_transformer.transformer.attn_layers.layers.17.1.wrap.net.0.proj.bias\", \"speech_transformer.transformer.attn_layers.layers.17.1.wrap.net.3.weight\", \"speech_transformer.transformer.attn_layers.layers.17.1.wrap.net.3.bias\", \"speech_transformer.transformer.attn_layers.layers.18.0.0.g\", \"speech_transformer.transformer.attn_layers.layers.18.1.wrap.to_q.weight\", \"speech_transformer.transformer.attn_layers.layers.18.1.wrap.to_k.weight\", \"speech_transformer.transformer.attn_layers.layers.18.1.wrap.to_v.weight\", \"speech_transformer.transformer.attn_layers.layers.18.1.wrap.to_out.weight\", \"speech_transformer.transformer.attn_layers.layers.18.1.wrap.to_out.bias\", \"speech_transformer.transformer.attn_layers.layers.19.0.0.g\", \"speech_transformer.transformer.attn_layers.layers.19.1.wrap.net.0.proj.weight\", \"speech_transformer.transformer.attn_layers.layers.19.1.wrap.net.0.proj.bias\", \"speech_transformer.transformer.attn_layers.layers.19.1.wrap.net.3.weight\", \"speech_transformer.transformer.attn_layers.layers.19.1.wrap.net.3.bias\", \"speech_transformer.transformer.attn_layers.layers.20.0.0.g\", \"speech_transformer.transformer.attn_layers.layers.20.1.wrap.to_q.weight\", \"speech_transformer.transformer.attn_layers.layers.20.1.wrap.to_k.weight\", \"speech_transformer.transformer.attn_layers.layers.20.1.wrap.to_v.weight\", \"speech_transformer.transformer.attn_layers.layers.20.1.wrap.to_out.weight\", \"speech_transformer.transformer.attn_layers.layers.20.1.wrap.to_out.bias\", \"speech_transformer.transformer.attn_layers.layers.21.0.0.g\", \"speech_transformer.transformer.attn_layers.layers.21.1.wrap.net.0.proj.weight\", \"speech_transformer.transformer.attn_layers.layers.21.1.wrap.net.0.proj.bias\", \"speech_transformer.transformer.attn_layers.layers.21.1.wrap.net.3.weight\", \"speech_transformer.transformer.attn_layers.layers.21.1.wrap.net.3.bias\", \"speech_transformer.transformer.attn_layers.layers.22.0.0.g\", \"speech_transformer.transformer.attn_layers.layers.22.1.wrap.to_q.weight\", \"speech_transformer.transformer.attn_layers.layers.22.1.wrap.to_k.weight\", \"speech_transformer.transformer.attn_layers.layers.22.1.wrap.to_v.weight\", \"speech_transformer.transformer.attn_layers.layers.22.1.wrap.to_out.weight\", \"speech_transformer.transformer.attn_layers.layers.22.1.wrap.to_out.bias\", \"speech_transformer.transformer.attn_layers.layers.23.0.0.g\", \"speech_transformer.transformer.attn_layers.layers.23.1.wrap.net.0.proj.weight\", \"speech_transformer.transformer.attn_layers.layers.23.1.wrap.net.0.proj.bias\", \"speech_transformer.transformer.attn_layers.layers.23.1.wrap.net.3.weight\", \"speech_transformer.transformer.attn_layers.layers.23.1.wrap.net.3.bias\", \"speech_transformer.transformer.attn_layers.rotary_pos_emb.inv_freq\", \"speech_transformer.transformer.norm.weight\", \"speech_transformer.transformer.norm.bias\". \n\tsize mismatch for text_emb.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([148, 512])."
     ]
    }
   ],
   "source": [
    "from tts_scores.clvp import CLVPMetric\n",
    "\n",
    "# Specify the absolute path to the clvp.pth file\n",
    "pretrained_path = './data/clvp.pth'  # Replace with the actual path to clvp.pth\n",
    "\n",
    "# Initialize the CLVP metric with the correct device (e.g., 'cuda') and the path to the model\n",
    "cv_metric = CLVPMetric(device='cuda', pretrained_path=pretrained_path)\n",
    "\n",
    "# Now you can use cv_metric as intended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv_metric' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# score = cv_metric.compute_fd('<path_to_your_generated_audio>, '<path_to_your_real_audio>')\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m score \u001b[39m=\u001b[39m cv_metric\u001b[39m.\u001b[39mcompute_fd(\u001b[39m'\u001b[39m\u001b[39m</home/irene/notebooks/audioFinetuned1.wav\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m/home/irene/notebooks/audioOrig1.wav\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cv_metric' is not defined"
     ]
    }
   ],
   "source": [
    "# score = cv_metric.compute_fd('<path_to_your_generated_audio>, '<path_to_your_real_audio>')\n",
    "score = cv_metric.compute_fd('/home/irene/notebooks/audioFinetuned1.wav', '/home/irene/notebooks/audioOrig1.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import librosa\n",
    "# import numpy as np\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# # Load your generated and original audio files\n",
    "# generated_audio, _ = librosa.load('/home/irene/notebooks/audioOrig1.wav', sr=None)\n",
    "# original_audio, _ = librosa.load('/home/irene/notebooks/audioFinetuned1.wav', sr=None)\n",
    "\n",
    "# # Extract audio features (e.g., Mel-frequency cepstral coefficients, MFCCs)\n",
    "# mfcc_generated = librosa.feature.mfcc(generated_audio, sr=44100)\n",
    "# mfcc_original = librosa.feature.mfcc(original_audio, sr=44100)\n",
    "\n",
    "# # Calculate cosine similarity between the MFCC features\n",
    "# similarity_score = cosine_similarity(mfcc_generated.T, mfcc_original.T)\n",
    "\n",
    "# # Print or use the similarity score as needed\n",
    "# print(f\"Cosine Similarity Score: {similarity_score[0][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MÉTRICAS CALCULADAS CON PYSEPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting https://github.com/schmiph2/pysepm/archive/master.zip\n",
      "  Downloading https://github.com/schmiph2/pysepm/archive/master.zip\n",
      "\u001b[2K     \u001b[32m/\u001b[0m \u001b[32m1.8 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m \u001b[33m0:00:00\u001b[0m0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pesq@ https://github.com/ludlows/python-pesq/archive/master.zip#egg=pesq (from pysepm==0.1)\n",
      "  Downloading https://github.com/ludlows/python-pesq/archive/master.zip\n",
      "\u001b[2K     \u001b[32m\\\u001b[0m \u001b[32m223.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m \u001b[33m0:00:00\u001b[0mm\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting SRMRpy@ https://github.com/jfsantos/SRMRpy/archive/master.zip#egg=SRMRpy (from pysepm==0.1)\n",
      "  Downloading https://github.com/jfsantos/SRMRpy/archive/master.zip\n",
      "\u001b[2K     \u001b[32m\\\u001b[0m \u001b[32m39.3 kB\u001b[0m \u001b[31m414.5 kB/s\u001b[0m \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pysepm==0.1) (1.22.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pysepm==0.1) (1.10.1)\n",
      "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from pysepm==0.1) (0.56.4+1.g5f1bc7084)\n",
      "Collecting pystoi (from pysepm==0.1)\n",
      "  Downloading pystoi-0.3.3.tar.gz (7.0 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->pysepm==0.1) (0.39.1)\n",
      "Collecting Gammatone@ https://github.com/detly/gammatone/archive/master.zip#egg=Gammatone (from SRMRpy@ https://github.com/jfsantos/SRMRpy/archive/master.zip#egg=SRMRpy->pysepm==0.1)\n",
      "  Downloading https://github.com/detly/gammatone/archive/master.zip\n",
      "\u001b[2K     \u001b[32m|\u001b[0m \u001b[32m59.4 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m \u001b[33m0:00:05\u001b[0mm\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting nose (from Gammatone@ https://github.com/detly/gammatone/archive/master.zip#egg=Gammatone->SRMRpy@ https://github.com/jfsantos/SRMRpy/archive/master.zip#egg=SRMRpy->pysepm==0.1)\n",
      "  Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.7/154.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: mock in /usr/local/lib/python3.10/dist-packages (from Gammatone@ https://github.com/detly/gammatone/archive/master.zip#egg=Gammatone->SRMRpy@ https://github.com/jfsantos/SRMRpy/archive/master.zip#egg=SRMRpy->pysepm==0.1) (5.0.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from Gammatone@ https://github.com/detly/gammatone/archive/master.zip#egg=Gammatone->SRMRpy@ https://github.com/jfsantos/SRMRpy/archive/master.zip#egg=SRMRpy->pysepm==0.1) (3.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->Gammatone@ https://github.com/detly/gammatone/archive/master.zip#egg=Gammatone->SRMRpy@ https://github.com/jfsantos/SRMRpy/archive/master.zip#egg=SRMRpy->pysepm==0.1) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->Gammatone@ https://github.com/detly/gammatone/archive/master.zip#egg=Gammatone->SRMRpy@ https://github.com/jfsantos/SRMRpy/archive/master.zip#egg=SRMRpy->pysepm==0.1) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->Gammatone@ https://github.com/detly/gammatone/archive/master.zip#egg=Gammatone->SRMRpy@ https://github.com/jfsantos/SRMRpy/archive/master.zip#egg=SRMRpy->pysepm==0.1) (4.39.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->Gammatone@ https://github.com/detly/gammatone/archive/master.zip#egg=Gammatone->SRMRpy@ https://github.com/jfsantos/SRMRpy/archive/master.zip#egg=SRMRpy->pysepm==0.1) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->Gammatone@ https://github.com/detly/gammatone/archive/master.zip#egg=Gammatone->SRMRpy@ https://github.com/jfsantos/SRMRpy/archive/master.zip#egg=SRMRpy->pysepm==0.1) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->Gammatone@ https://github.com/detly/gammatone/archive/master.zip#egg=Gammatone->SRMRpy@ https://github.com/jfsantos/SRMRpy/archive/master.zip#egg=SRMRpy->pysepm==0.1) (9.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->Gammatone@ https://github.com/detly/gammatone/archive/master.zip#egg=Gammatone->SRMRpy@ https://github.com/jfsantos/SRMRpy/archive/master.zip#egg=SRMRpy->pysepm==0.1) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->Gammatone@ https://github.com/detly/gammatone/archive/master.zip#egg=Gammatone->SRMRpy@ https://github.com/jfsantos/SRMRpy/archive/master.zip#egg=SRMRpy->pysepm==0.1) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->Gammatone@ https://github.com/detly/gammatone/archive/master.zip#egg=Gammatone->SRMRpy@ https://github.com/jfsantos/SRMRpy/archive/master.zip#egg=SRMRpy->pysepm==0.1) (1.16.0)\n",
      "Building wheels for collected packages: pysepm, pesq, pystoi, SRMRpy, Gammatone\n",
      "  Building wheel for pysepm (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pysepm: filename=pysepm-0.1-py3-none-any.whl size=24305 sha256=4c7cbdf658cadf9df8a2d33f6c977d31dfbf9ef1c522dd87c67e803467b59fdd\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-mx251opc/wheels/90/39/b8/6307733bc5df0a99568f88ac06d98352c79425f1045524c156\n",
      "  Building wheel for pesq (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pesq: filename=pesq-0.0.4-cp310-cp310-linux_x86_64.whl size=224067 sha256=cf088986d974c7ab035bfd172a138f79f21fd525f1312f92c2776aab8d144ae1\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-mx251opc/wheels/3d/7d/9f/9e1d63dc212910a515ac320c8ebfa8467839b0fef3fc1bb57f\n",
      "  Building wheel for pystoi (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pystoi: filename=pystoi-0.3.3-py2.py3-none-any.whl size=7792 sha256=5fb94bb03da3c01bc21769ca69b22ec641537422ed4ce54c18302760fcc1cda9\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-mx251opc/wheels/3b/ca/9e/5b5d6e5e109322303b50d21918ad2bd7d50a2a0775c11e08e8\n",
      "  Building wheel for SRMRpy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for SRMRpy: filename=SRMRpy-1.0-py3-none-any.whl size=9390 sha256=17f8dc731a906e1e6a8e5505239804d4520714ec36e15bdd93218f08f8493e26\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-mx251opc/wheels/9b/51/3a/68d1b8f3d0c9954416b967547e478c8556f3473af1e5df96fb\n",
      "  Building wheel for Gammatone (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for Gammatone: filename=Gammatone-1.0-py3-none-any.whl size=21787 sha256=fcbf64a4292672acf0c844c6efeda47f3db8aae7de7acab63edb151c537c4416\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-mx251opc/wheels/f7/33/eb/6c0c33ef9ad7fcd7e1355e77994e88a22ce62d68a417260660\n",
      "Successfully built pysepm pesq pystoi SRMRpy Gammatone\n",
      "Installing collected packages: pesq, nose, pystoi, Gammatone, SRMRpy, pysepm\n",
      "Successfully installed Gammatone-1.0 SRMRpy-1.0 nose-1.3.7 pesq-0.0.4 pysepm-0.1 pystoi-0.3.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install https://github.com/schmiph2/pysepm/archive/master.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import wavfile\n",
    "from scipy.signal import resample\n",
    "import sys\n",
    "sys.path.append(\"../\") \n",
    "import pysepm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for /home/irene/notebooks/muestrasModelosBase/audio_modelo_base_1.wav:\n",
      "fwSNRseg: 2.7873186064696553\n",
      "SNRseg: -3.397573660436624\n",
      "LLR: 1.9719273110635167\n",
      "WSS: 98.31711705773556\n",
      "Cepstrum Distance: 9.804188018349352\n",
      "STOI: 0.05917911758771761\n",
      "CSII: (0.0, 0.0, 0.0)\n",
      "BSD: 2299267594.1963573\n",
      "\n",
      "\n",
      "Metrics for /home/irene/notebooks/muestrasModelosBase/audio_modelo_base_2.wav:\n",
      "fwSNRseg: 3.3267717360769318\n",
      "SNRseg: -3.3908011467302246\n",
      "LLR: 1.9556438871058446\n",
      "WSS: 95.37886867601392\n",
      "Cepstrum Distance: 9.797060584897759\n",
      "STOI: 0.11277212699886188\n",
      "CSII: (0.0, 4.822978402682473e-05, 0.007052310347891178)\n",
      "BSD: 460804545.3298916\n",
      "\n",
      "\n",
      "Metrics for /home/irene/notebooks/muestrasModelosBase/audio_modelo_base_3.wav:\n",
      "fwSNRseg: 2.765908126817959\n",
      "SNRseg: -3.414098847116286\n",
      "LLR: 1.9505945042540762\n",
      "WSS: 95.54108886873074\n",
      "Cepstrum Distance: 9.824989104957096\n",
      "STOI: 0.04685633646868701\n",
      "CSII: (0.0, 0.00010091247406352087, 0.0)\n",
      "BSD: 2772008744.813369\n",
      "\n",
      "\n",
      "Metrics for /home/irene/notebooks/muestrasModelosFinetunedFastpitch/audio_modelo_Finetuned_FastPitch_1.wav:\n",
      "fwSNRseg: 3.9795532508123257\n",
      "SNRseg: -2.7430966271780823\n",
      "LLR: 1.8498366819343994\n",
      "WSS: 84.90540912512567\n",
      "Cepstrum Distance: 9.281573634829021\n",
      "STOI: 0.13573360522569153\n",
      "CSII: (0.0, 0.0003426942772832556, 0.0014873645554161893)\n",
      "BSD: 33552977.70956176\n",
      "\n",
      "\n",
      "Metrics for /home/irene/notebooks/muestrasModelosFinetunedFastpitch/audio_modelo_Finetuned_FastPitch_2.wav:\n",
      "fwSNRseg: 4.085090938801377\n",
      "SNRseg: -2.456932460603226\n",
      "LLR: 1.8172663535170421\n",
      "WSS: 86.04003485523795\n",
      "Cepstrum Distance: 9.176588909820227\n",
      "STOI: 0.14154882453522932\n",
      "CSII: (0.0, 0.00036247526699621405, 0.0014174547948261357)\n",
      "BSD: 5856353.358065309\n",
      "\n",
      "\n",
      "Metrics for /home/irene/notebooks/muestrasModelosFinetunedFastpitch/audio_modelo_Finetuned_FastPitch_3.wav:\n",
      "fwSNRseg: 4.134844282236668\n",
      "SNRseg: -2.749830188854514\n",
      "LLR: 1.832913481613219\n",
      "WSS: 86.7719135413858\n",
      "Cepstrum Distance: 9.20474367318063\n",
      "STOI: 0.11924018395152147\n",
      "CSII: (0.0, 0.0, 0.011462700014767206)\n",
      "BSD: 19790573.19636807\n",
      "\n",
      "\n",
      "Metrics for /home/irene/notebooks/muestrasModelosFinetunedHifiGAN/audio_modelo_Finetuned_HifiGAN_1.wav:\n",
      "fwSNRseg: 3.1209356548468112\n",
      "SNRseg: -3.2669830326968214\n",
      "LLR: 1.9329560839523279\n",
      "WSS: 95.2300307626796\n",
      "Cepstrum Distance: 9.787663964862064\n",
      "STOI: 0.06516623922593079\n",
      "CSII: (0.0, 0.0001433208426227705, 0.0)\n",
      "BSD: 22463302875.56421\n",
      "\n",
      "\n",
      "Metrics for /home/irene/notebooks/muestrasModelosFinetunedHifiGAN/audio_modelo_Finetuned_HifiGAN_2.wav:\n",
      "fwSNRseg: 2.8334888827038673\n",
      "SNRseg: -3.0194751483182443\n",
      "LLR: 1.812628648060216\n",
      "WSS: 88.43110367077149\n",
      "Cepstrum Distance: 9.265963844966397\n",
      "STOI: 0.051301349636211566\n",
      "CSII: (0.0, 0.0, 0.0)\n",
      "BSD: 28757804441.119602\n",
      "\n",
      "\n",
      "Metrics for /home/irene/notebooks/muestrasModelosFinetunedHifiGAN/audio_modelo_Finetuned_HifiGAN_3.wav:\n",
      "fwSNRseg: 2.959978348302992\n",
      "SNRseg: -3.1095657696417183\n",
      "LLR: 1.7944930224026296\n",
      "WSS: 94.72899134067559\n",
      "Cepstrum Distance: 9.235630955389464\n",
      "STOI: 0.12847681890076532\n",
      "CSII: (0.0, 0.0, 0.0)\n",
      "BSD: 80947561617.71901\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy.io import wavfile\n",
    "from scipy.signal import resample\n",
    "import pysepm\n",
    "\n",
    "# List of audio file paths\n",
    "audio_files = [\n",
    "    '/home/irene/notebooks/muestrasModelosBase/audio_modelo_base_1.wav',\n",
    "    '/home/irene/notebooks/muestrasModelosBase/audio_modelo_base_2.wav',\n",
    "    '/home/irene/notebooks/muestrasModelosBase/audio_modelo_base_3.wav',\n",
    "    '/home/irene/notebooks/muestrasModelosFinetunedFastpitch/audio_modelo_Finetuned_FastPitch_1.wav',\n",
    "    '/home/irene/notebooks/muestrasModelosFinetunedFastpitch/audio_modelo_Finetuned_FastPitch_2.wav',\n",
    "    '/home/irene/notebooks/muestrasModelosFinetunedFastpitch/audio_modelo_Finetuned_FastPitch_3.wav',\n",
    "    '/home/irene/notebooks/muestrasModelosFinetunedHifiGAN/audio_modelo_Finetuned_HifiGAN_1.wav',\n",
    "    '/home/irene/notebooks/muestrasModelosFinetunedHifiGAN/audio_modelo_Finetuned_HifiGAN_2.wav',\n",
    "    '/home/irene/notebooks/muestrasModelosFinetunedHifiGAN/audio_modelo_Finetuned_HifiGAN_3.wav',\n",
    "    '/home/irene/notebooks/muestrasModelosBase/audioOrigBase.wav'\n",
    "]\n",
    "\n",
    "# Load the reference audio signal\n",
    "fs_orig, base_speech = wavfile.read(audio_files[-1])  # Assuming the reference audio is the last one in the list\n",
    "\n",
    "# Initialize dictionaries to store metrics for each audio file\n",
    "metrics_dict = {}\n",
    "\n",
    "# Loop through the audio files and compute metrics, but terminate after the third iteration\n",
    "for i, audio_file in enumerate(audio_files):\n",
    "    fs, orig_speech = wavfile.read(audio_file)\n",
    "\n",
    "    # Check and ensure matching sampling frequencies\n",
    "    if fs != fs_orig:\n",
    "        orig_speech = resample(orig_speech, len(base_speech))\n",
    "        fs = fs_orig\n",
    "\n",
    "    # Check and ensure matching lengths\n",
    "    if len(orig_speech) != len(base_speech):\n",
    "        min_length = min(len(orig_speech), len(base_speech))\n",
    "        orig_speech = orig_speech[:min_length]\n",
    "        base_speech = base_speech[:min_length]\n",
    "\n",
    "    # Compute the metrics for the current audio file\n",
    "    # Métrica fwSNRseg\n",
    "    fwSNRseg = pysepm.fwSNRseg(orig_speech, base_speech, fs)\n",
    "    # Métrica SNRreg\n",
    "    SNRseg = pysepm.SNRseg(orig_speech, base_speech, fs)\n",
    "    # Métrica LLR\n",
    "    llr = pysepm.llr(orig_speech, base_speech, fs)\n",
    "    # Métrica WSS\n",
    "    wss = pysepm.wss(orig_speech, base_speech, fs)\n",
    "    # Métrica Cepstrum Distance\n",
    "    cepstrum_distance = pysepm.cepstrum_distance(orig_speech, base_speech, fs)\n",
    "    # Métrica STOI\n",
    "    stoi = pysepm.stoi(orig_speech, base_speech, fs)\n",
    "    # Métrica CSII\n",
    "    csii = pysepm.csii(orig_speech, base_speech, fs)\n",
    "    # Métrica BSD\n",
    "    bsd = pysepm.bsd(orig_speech, base_speech, fs)\n",
    "\n",
    "    # Store the metrics in the dictionary\n",
    "    metrics_dict[f'Audio_{i + 1}'] = {\n",
    "        'fwSNRseg': fwSNRseg,\n",
    "        'SNRseg': SNRseg,\n",
    "        'LLR': llr,\n",
    "        'WSS': wss,\n",
    "        'Cepstrum Distance': cepstrum_distance,\n",
    "        'STOI': stoi,\n",
    "        'CSII': csii,\n",
    "        'BSD': bsd\n",
    "    }\n",
    "\n",
    "    # Print the metrics for the current audio file\n",
    "    print(f\"Metrics for {audio_file}:\")\n",
    "    for metric_name, value in metrics_dict[f'Audio_{i + 1}'].items():\n",
    "        print(f\"{metric_name}: {value}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Break out of the loop after the third iteration\n",
    "    # finBucle = len(audio_files)-1\n",
    "    if i == 8:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último se realiza la evaluación perceptual. En ella se van a ejecutar los dos mejores modelos. Se utilizarán las 6 frases de la evaluación perceptal. Habrá un total de 12 frases, 2 por cada frase de test. El cuaderno donde se puede ejecutar se llama evaluateEvaluacionPerceptual.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
